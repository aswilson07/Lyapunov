\documentclass[11pt]{article}
\usepackage{tikz}
\usepackage{fullpage, url,amsmath,amsfonts,amssymb,mathtools,mathrsfs,graphicx,  algorithm, float, sansmath,epstopdf,color,caption,enumitem,tabularx}
\usepackage[final]{pdfpages}
\usepackage{amsthm}
\usetikzlibrary{automata,topaths}
\usetikzlibrary{decorations.pathreplacing,shapes.misc}
\usepackage{fancyhdr}

%\renewcommand{\chaptername}{Lecture}

\usepackage{multirow}
\usetikzlibrary{calc,arrows}
\theoremstyle{plain}
\newtheorem{definition}{Definition}
\newtheorem{remark}{Remark}
\newtheorem{properties}{Properties}
\newtheorem{example}{Example}
\newtheorem{theorem}{Theorem}
%\newtheorem{proposition}{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{fact}[theorem]{Fact}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{observation}{Observation}
\usepackage{blindtext}
\def \endprf{\hfill {\vrule height6pt width6pt depth0pt}\medskip}
%\newenvironment{proof}{\noindent {\bf Proof} }{\endprf\par}
\usepackage[colorlinks,linkcolor=blue,citecolor=red]{hyperref}
\setlength{\tabcolsep}{18pt}
\newcommand{\vct}[1]{#1}
\newcommand{\mtx}[1]{#1}
\newcommand{\sfT}{\mathsf{T}}  % tangent space


\newcommand{\T}{\mathbb{T}}
\newcommand{\R}{{\mathbb R}}
\newcommand{\E}{{\mathcal E}}
\newcommand{\Z}{{\mathbb Z}}
\newcommand{\Q}{{\mathbb Q}}
\newcommand{\C}{{\mathbb C}}
\newcommand{\N}{{\mathbb N}}
\newcommand{\X}{{\mathcal X}}
\newcommand{\tx}{\tilde{x}}
\newcommand{\ty}{\tilde{y}}
\newcommand{\tu}{\tilde{u}}
\newcommand{\tz}{\tilde{z}}
\newcommand{\TxX}{\sfT_x\X}  % tangent space

\newcommand{\A}{\mathcal{A}}

\newcommand{\loss}{\operatorname*{loss}}
\newcommand{\minimize}{\mathrm{minimize}}
\newcommand{\st}{\mathrm{subject to}}

\newcommand{\todo}[1]{\vspace{5 mm}\par \noindent \marginpar{\textsc{ToDo}}
\framebox{\begin{minipage}[c]{0.95 \columnwidth} \tt #1
\end{minipage}}\vspace{5 mm}\par}

\newcommand{\insertrep}[1]{%
\hspace*{-2.4cm}
\fbox{\includegraphics[page=1,scale=0.4]{#1}}
\includepdf[scale=0.4,pages=1-,frame]{#1}
}

\setlength{\oddsidemargin}{0pt}
\setlength{\evensidemargin}{0pt}
\setlength{\textwidth}{6.5in}
\setlength{\topmargin}{0in}
\setlength{\textheight}{8.5in}

\begin{document}
\title{A Lyapunov Analysis of Momentum Methods \\ in  Optimization}
%\title{A Lyapunov Perspective on Momentum Methods \\ in  Optimization}
\date{\today}
\author{Ashia C. Wilson \qquad Benjamin Recht \qquad Michael I. Jordan\\ \\
University of California, Berkeley
}
\date{\today}
\maketitle
\begin{abstract}
Momentum methods play a central role in optimization. Several momentum methods are provably optimal, and all use a technique called {\em estimate sequences} to analyze their convergence properties.  The technique of estimate sequences has long been considered difficult to understand, leading many researchers to generate alternative, ``more intuitive'' methods and analyses. In this paper we show there is an equivalence between the technique of estimate sequences and a family of Lyapunov functions in both continuous and discrete time. This framework allows us to develop a simple and unified analysis of many existing momentum algorithms, introduce several new algorithms, and most importantly, strengthen the connection between algorithms and continuous-time dynamical systems. 
\end{abstract}

\section{Introduction}

Momentum is a powerful heuristic for accelerating the convergence of optimization methods.    One can intuitively ``add momentum'' to a method by adding to the current step a weighted version of the previous step, encouraging the method to move along search directions that had been previously seen to be fruitful.  Such methods were first popularized by Polyak~\cite{Polyak1964}, and have been employed in many practical optimization solvers.  In particular, since the 1980s, momentum methods have been popular in neural networks as a way to accelerate the backpropagation algorithm.  The intuition is that momentum allows local search to avoid ``long ravines'' and ``sharp curvatures'' in the sublevel sets of cost functions~\cite{Rumelhardt}.

Polyak motivated momentum methods by an analogy to a ``heavy ball'' moving in a potential well defined by the cost function.  However, Polyak's physical intuition was exceedingly difficult to make rigorous.  For quadratic costs, Polyak was able to provide an eigenvalue argument that showed that his Heavy Ball method required no more iterations than the method of conjugate gradients~\cite{Polyak1964}.  
Indeed, when applied to positive-definite quadratic cost functions, Polyak's Heavy Ball method is equivalent to Chebyshev's Iterative Method~\cite{Chebyshev}.   Despite its intuitive elegance, however, Polyak's eigenvalue analysis does not apply globally for general convex cost functions. Indeed, Lessard~\emph{et al.} derived a simple one-dimensional counterexample where the standard Heavy Ball method does not converge~\cite{Lessard14}.  

In order to make momentum methods rigorous, a different approach was required.  In celebrated work, Nesterov devised a general scheme to accelerate convex optimization methods, achieving optimal running times under oracle models in convex programming~\cite{Nesterov04}. However, to achieve such general applicability, Nesterov's proof techniques abandoned the physical intuition of Polyak~\cite{Nesterov04}; in lieu of differential equations and potential functions, Nesterov devised the method of \emph{estimate sequences} to verify the correctness of these momentum-based methods.  Researchers have struggled to understand the intuition and underpinnings of the estimate sequence methodology since Nesterov's initial papers.  The associated proof techniques are often viewed as an ``algebraic trick.''  

To overcome the lack of intuition for the estimate sequence technique, several authors have recently proposed schemes to achieve acceleration without appealing to estimate sequences~\cite{Fazel, BubeckLeeSingh15, Lessard14, DroriTeboulle13}.  One promising general approach to the analysis of acceleration has been to analyze the continuous-time limit of accelerated methods and to show that the stability of the resulting ODEs can be verified via a Lyapunov function~\cite{SuBoydCandes14,Krichene15,Acceleration}.  However, these methods stop short of providing principles for deriving a discrete-time optimization algorithm from a continuous-time ODE.  There are many ways to discretize ODEs, but not all of them give rise to convergent methods or to acceleration.  Indeed, for unconstrained optimization on Euclidean spaces in the setting where the objective is strongly convex, Polyak's Heavy Ball method and Nesterov's accelerated gradient descent have the same continuous-time limit.

In this paper, we propose a bridge between continuous-time limits and discrete-time algorithms.  Our method takes, as its primary object of study, a general Lyapunov function for momentum methods in continuous time.  Through a diverse set of examples, we demonstrate how the proofs of convergence of momentum methods can be understood as bounding discretization errors of the Lyapunov function when moving to discrete time.  In particular, we show how the discretization of the associated continuous-time ODE needs to be performed in such a way such that the Lyapunov function remains valid when transitioning to discrete time.
Using this technique, we provide a simple, direct proof of the convergence of Nesterov's method for strongly convex functions in Euclidean spaces.  Our proof makes clear why only certain parameterizations of the continuous ODEs lead to valid discrete-time methods.  Moreover, we provide new perspectives on the need for the extragradient step inside Nesterov's method which does not appear in Polyak's method.  
Finally, we show there is a striking equivalence between estimate sequences and Lyapunov functions.  In continuous and discrete time, estimate sequences can be derived directly from the Lyapunov function and vice versa.  We show that the associated continuous-time estimate sequence can also be directly discretized to give standard estimate-sequence based proofs of momentum methods.  This clarifies how Nesterov's ``algebraic trick'' is closely related to invariant reduction proofs that are common in algorithm analysis.

The paper proceeds as follows.  We first describe the related work in viewing optimization algorithms as discretizations of continuous-time dynamical systems.  We then introduce the Lyapunov functions that we will use throughout the paper to demonstrate convergence of algorithms.  In particular, we will introduce time-varying Lyapunov functions that not only establish algorithm convergence but additionally verify a \emph{convergence rate}.  
 We then turn to various momentum-based methods, providing a general Lyapunov analysis for Nesterov's accelerated method, the quasi-monotone subgradient method, the conditional gradient method, and a few novel algorithms.  We pay particular attention to the strongly convex case in Euclidean space, highlighting some unique properties of this setup.  Finally, we describe the connection between estimate sequences and Lyapunov functions and discuss directions for future investigation.
 
 We try to include as many of the mathematical derivations as possible in the main body of the paper.  Indeed, all proofs will follow a similar flow of analysis using a discretized Lyapunov function.  However, as is often the case, the specifics of individual algorithms often require involved algebraic calculations to fully verify.   In the spirit of reproducibility, we provide detailed proofs of convergence rates of all of the algorithms we discuss in the Appendix.

\section{The Dynamical View of Momentum Methods}

\paragraph{Problem setting}
This paper is concerned with the class of constrained optimization problems
\begin{align}\label{eq:main-problem}
\min_{x \in \X} \; f(x),
\end{align}
where $\X \subseteq \R^d$ is a closed convex set and $f \colon \X \to \R$ is a continuously differentiable convex function. 
We use the standard Euclidean norm $\|x\| = \langle x,x \rangle^{1/2}$ throughout.
We consider the general non-Euclidean setting in which the space $\X$ is endowed with a distance-generating function $h \colon \X \to \R$ that is convex and essentially smooth (i.e., $h$ is continuously differentiable in $\X$, and $\|\nabla h(x)\|_* \to \infty$ as $\|x\| \to \infty$). The function $h$ can be used to define a measure of distance in $\X$ via its Bregman divergence:
\begin{align*}
D_h(y,x) = h(y) - h(x) - \langle \nabla h(x), y-x \rangle,
\end{align*}
which is nonnegative since $h$ is convex. 
The \emph{Euclidean setting} is obtained when $h(x) = \frac{1}{2} \|x\|^2$.

We denote a discrete-time sequence in lower case, e.g., $x_k$ with $k \ge 0$ an integer. We denote a continuous-time curve in upper case, e.g., $X_t$ with $t \in \R$. An over-dot means derivative with respect to time, i.e., $\dot X_t = \frac{d}{dt} X_t$.


\paragraph{Momentum methods}
The intuition for solving problem~\eqref{eq:main-problem} with momentum methods comes from viewing optimization algorithms as discrete-time approximations to ODEs. In particular, the second-order ODE:
 \begin{align}\label{eq:heavy-ball-ode}
 m \frac{d^2x}{dt^2}& = - \nabla f(x) - m \dot \gamma_t \frac{dx}{dt}
 \end{align}
corresponds to the equations of motion of an object with mass $m$ in the presence of  friction or viscosity, captured by the derivative of a function $\gamma_t$, and a potential $f$. The trajectories of this dynamical system will tend to have directions of motion that persist over time; however, for some values of $\gamma_t$, the dynamics will converge to the minimum of $f$.  In the limit as the mass goes to zero, and letting $\gamma_t \propto t/m$, we recover the gradient-flow dynamics, which is the continuous-time limit of gradient descent~\cite{Acceleration}.  Thus, the mass term here serves to \emph{accelerate} the progress towards the bottom of the well. 

In seminal work, Polyak defined and analyzed the Heavy Ball method as an application of Euler's method to the dynamical system~\eqref{eq:heavy-ball-ode}, when $\gamma_t$ is linear.  He was able to show \emph{local convergence} of this method, but was unable to prove global convergence.  Moreover, his local convergence result required the assumption that $f$ is strongly convex in the neighborhood of the associated stationary point.

Until recently, this ODE perspective on optimization algorithms was largely abandoned in favor of Nesterov's estimate sequence framework.  However, there has been a recent resurgence in interest in ODE-based analyses.  Su, Boyd and Cand\`es~\cite{SuBoydCandes14}, showed that the continuous-time limit of Nesterov's accelerated gradient descent method corresponds to the following second-order ODE, 
 \begin{equation}\label{Eq:SuBoydCandes}
 \ddot X_t + \frac{r}{t} \dot X_t + \nabla f(X_t) = 0,\,\,\,\,r \geq 3.
 \end{equation}
This is equivalent to~\eqref{eq:heavy-ball-ode}, where the object being modeled has unit mass and the damping term is logarithmic, $\gamma_t = r\log t$.  They also established a convergence rate of $O(1/t^2)$ for this continuous-time system via a Lyapunov function argument. This matches the $O(1/\epsilon k^2)$ convergence rate of accelerated gradient descent, given the discretization $t= \sqrt\epsilon k$. Thus, the convergence rate is maintained in passing from continuous time to discrete time. Two things remain unclear from their analysis however:  they introduce an additional sequence $\{y_k\}_{k=1}^\infty$ while discretizing the ODE~\cite[Sec 2]{Acceleration},  which does not correspond to a straightforward discretization technique; and they do not demonstrate what connection---if any---there is between the Lyapunov argument used to analyze the continuous-time dynamics and the technique of estimate sequences used to analyze the discrete-time algorithm. 

Krichene, Bartlett and Bayen~\cite{Krichene15} derived a modified accelerated mirror descent algorithm using a ``discretized'' Lyapunov function. However, their proof targeted a different algorithm than the algorithm in Nesterov's original work~\cite{Nesterov05}. In particular, their analysis entailed an additional smoothness assumption which is not necessary for the original algorithm~\cite{Nesterov05}, or for the original proof technique using estimate sequences.\footnote{In particular, they assume $\frac{\ell }{2} \|x-y\|^2 \leq  D_h(x,y) \leq \frac{L}{2}\|x-y\|^2$%, so that for all intensive purposes, their setting is given by the norm
. This additional assumption greatly simplifies the proof. However, only the assumption of an upper bound, $\frac{\ell }{2} \|x-y\|^2 \leq  D_h(x,y)$,  is needed for the most general form of accelerated methods.} See also Nemirovski and Yudin's original work~\cite{NemirovskiiYudin} for a continuous-time perspective on mirror descent and a Lyapunov analysis of convergence.

Wibisono, Wilson, and Jordan~\cite{Acceleration} introduced a class of dynamical systems whose discretizations give rise to a general family of accelerated gradient algorithms, including accelerated gradient and mirror descent. These algorithms are obtained as a discretization of the following second-order ODE:
 \begin{align} \label{Eq:ELBreg} 
\frac{d}{dt} \nabla h(X_t + e^{-\alpha_t} \dot X_t) = -e^{\alpha_t+\beta_t} \nabla f(X_t)\,,
\end{align}
where $h$ is a strongly convex function associated with the geometry of the space and where $\alpha_t$ and $\beta_t$ are smooth functions of time.  In the case where $\alpha_t=-\log m$, and $\beta_t = \log m$,  the continuous-time limit $m \rightarrow 0$ of this ODE recovers the mirror descent dynamics~\cite{Acceleration}, with $h$ playing the role of the distance-generating function.  Indeed, the original motivation for mirror descent by Nemirovski and Yudin was from this continuous perspective, and they demonstrated a Lyapunov argument for the convergence of the continuous-time dynamics (see Section 3.1 of Nemirovski and Yudin~\cite{NemirovskiiYudin}).

Wibisono \emph{et al.}~\cite{Acceleration} imposed the following conditions on the functions $\alpha_t$, $\beta_t$, and $\gamma_t$:
\begin{subequations}\label{Eq:IdeSca}
\begin{align}
\dot \beta_t \,&\leq\, e^{\alpha_t}   \label{Eq:IdeScaBet} \\
\dot \gamma_t \,&=\, e^{\alpha_t}  \label{Eq:IdeScaGam},
\end{align}
\end{subequations}
referring to these conditions as~\emph{ideal scaling}, and showing their necessity for achieving accelerated rates. These conditions will also play an important role in the present work.  Wibisono {\em et al.}~\cite{Acceleration} also require that  $f$ and $h$ be differentiable and convex, and that $h$ be strictly convex. Under the ideal scaling, they demonstrated that the momentum ODE~\eqref{Eq:ELBreg} is the Euler-Lagrange equation for a functional that they referred to as the {\em Bregman Lagrangian}. This means that the family of accelerated gradient dynamics---like the gradient flows which generate gradient methods---can be understood as being generated from a {\em variational principle}. They also presented a Lyapunov function to demonstrate a convergence rate of $O(e^{-\beta_t})$ for the momentum dynamics~\eqref{Eq:ELBreg}. However, they did not provide a Lyapunov analysis of the discrete-time algorithm, nor did they make explicit the connection between the Lyapunov function and the estimate sequence technique. 

\section{Rate-Generating Lyapunov Functions}
Lyapunov's method~\cite{Lyapunov} is based on the idea of constructing a positive definite quantity $\E: \X \rightarrow \R$ which decreases along the trajectories of the dynamical system $\dot X_t = v(X_t)$:
\begin{equation*}
\dot \E(X_t) =\langle \nabla \E(X_t), v(X_t)\rangle < 0.
\end{equation*}
The existence of such a \emph{Lyapunov function} guarantees that the dynamical system converges: if the function is positive yet strictly decreasing along all trajectories, then the dynamical system must eventually approach a region where $\E(X)$ is minimal.  If this region coincides with the stationary points of the dynamics, then all trajectories must converge to a stationary point.

In this paper, we are interested in obtaining Lyapunov functions for dynamical systems designed for optimization.  In optimization, we are typically interested in the~\emph{rate of convergence} of a method, not mere convergence.  To facilitate such rate analysis, we study time-dependent Lyapunov functions.  Though it is unconventional to have Lyapunov functions depend explicitly on time, our Lyapunov function is parameterized explicitly by $\alpha_t$, $\beta_t$, and $\gamma_t$, reflecting the dependence of the dynamics~\eqref{Eq:ELBreg} on these functions.

The family of dynamics that obey the ideal scaling constraints~\eqref{Eq:IdeSca} is invariant under time reparameterizations.  Such time-invariance properties provide more flexibility in algorithm design and analysis, as we will see throughout this paper.

We begin by presenting time-varying Lyapunov functions that can verify the convergence rates of continuous-time momentum flows, and we then turn to the corresponding discrete-time analysis.

\paragraph{General accelerated mirror descent} In Wibisono {\em et al.}~\cite{Acceleration}, the following Lyapunov function:
\begin{align}\label{Eq:E}
\E_t \,=\, e^{\beta_t} (f(X_t)-f(x^*)) + D_h\left(x^*, \, X_t + e^{-\alpha_t} \dot X_t\right),
\end{align}
was introduced to analyze the flow that forms the basis for accelerated mirror descent.
In Section~\ref{Sec:MomLyap}, we show how this Lyapunov function emerges from a general framework for analyzing momentum dynamics; furthermore, we show how to discretize the Lyapunov function and thereby obtain a tool which can be used to analyze a variety of algorithms that are discretizations of momentum dynamics.

\paragraph{Accelerated gradient descent (strong convexity)}
In this paper, we show that the following function 
   \begin{equation}\label{eq:general-sc-lyap}
  \E_t =  e^{\beta_t} \left(f(X_t) - f(x^\ast) + \mu D_h\Big(x^\ast ,X_t + e^{-\alpha_t} \dot X_t\Big)\right),
 \end{equation}
is a Lyapunov function for a family of momentum dynamics when $f$ is $\mu$-strongly convex ($D_f(y,x) \geq \mu D_h(y,x)$).  In Section~\ref{Sec:StrongConv}, we derive this Lyapunov function and show how it can be used to demonstrate a linear rate of convergence for the family of accelerated gradient descent dynamics.  (For simplicity, we focus on the setting in which $h = \frac{1}{2}\|\cdot\|^2$ is Euclidean.) We also show that a particular discretization of the family of dynamics provides an algorithm for which a commensurate discretization of~\eqref{eq:general-sc-lyap} is a Lyapunov function.


Note that in both the strongly convex and weakly convex setting, the structure of the Lyapunov function allows us to derive a rate of convergence for the optimality gap $f(X_t) - f(x^\ast)$. In particular, the Lyapunov property $\E_t \leq \E_0$ implies 
\begin{equation}\label{eq:Guarantee1}
f(X_t) - f(x^\ast) \leq \frac{e^{\beta_0}(f(X_0) - f(x^\ast)) + D_h(x^\ast, X_0 + e^{-\alpha_0} \dot X_0)}{e^{\beta_t}},
\end{equation}
and 
\begin{equation}
f(X_t) - f(x^\ast) \leq \frac{e^{\beta_0}(f(X_0) - f(x^\ast) +\mu D_h(x^\ast ,X_0 -e^{-\alpha_0} \dot X_0))}{e^{\beta_t}},
\end{equation}
for~\eqref{Eq:E} and \eqref{eq:general-sc-lyap} respectively. The typical convention is that the dynamics start from rest $\dot X_0 = 0$. Notice that we obtain the same rates of convergence in these two settings. However, the family of dynamics corresponding to the Lyapunov function in the strongly convex setting exploits the strong convexity of $f$, and this allows us to obtain a tighter bound in discrete time.


\section{Momentum Methods}
\label{Sec:MomLyap}
In this section, we introduce two families of momentum dynamics, both of which are generated by functionals defined on continuous-time curves. We illustrate how to derive a Lyapunov function from these dynamics. Next, we show how these dynamics can be discretized to obtain optimization algorithms. The same discrete-time mappings can be applied to the Lyapunov function, resulting in a tool which can be used to analyze the discrete-time algorithms. We provide examples of this procedure for the quasi-monotone subgradient algorithm, accelerated gradient descent, the conditional gradient algorithm, and several other novel algorithms. 
 \subsection{Lyapunov analysis}
We study dynamics that are based on the Euler-Lagrange equation \eqref{Eq:ELBreg} in the setting where the ideal scaling~\eqref{Eq:IdeScaBet} holds with equality~$\dot \beta_t = e^{\alpha_t}$. This implies that the Bregman Lagrangian is parameterized by $\beta_t$, which determines the convergence rate:
 \begin{subequations}\label{Eq:EL}
 \begin{align}
 Z_t &= X_t + \frac{1}{\dot \beta_t} \dot X_t \label{Eq:ELZ},\\
 \frac{d}{dt}\nabla h(Z_t) &= -\dot \beta_t e^{\beta_t} \nabla f(X_t)\label{Eq:ELH}.
 \end{align}
 \end{subequations}
We begin by demonstrating how to derive the Lyapunov function~\eqref{Eq:E} for the momentum dynamics~\eqref{Eq:EL}; this derivation is similar in spirit to the Lyapunov analysis of mirror descent by Nemirovski and Yudin.  We have:
\begin{subequations}
\begin{align*}
 \frac{d}{dt}D_h\left(x, Z_t\right)  &= \frac{d}{dt}\left( h(x) - h(Z_t) - \langle \nabla h(Z_t), x- Z_t\rangle\right)  \notag\\
& = - \langle \nabla h(Z_t), \dot Z_t\rangle -\left\langle \frac{d}{dt} \nabla h(Z_t),x - Z_t\right\rangle  + \langle \nabla h(Z_t), \dot Z_t\rangle \notag\\ 
 &= -\left\langle \frac{d}{dt} \nabla h\left(Z_t\right), x - Z_t\right\rangle. \, \notag%\label{eq:apply-identity}\\
 \end{align*}
 \end{subequations}
Using this identity, we obtain the following argument:
%\begin{small}
\begin{subequations}\label{Eq:LyapAnal}
\begin{align}
 \frac{d}{dt}D_h\left(x, Z_t\right)  
 &= -\left\langle \frac{d}{dt} \nabla h\left(Z_t\right), x - Z_t\right\rangle \, \notag\\%\label{eq:apply-identity}\\
&=   \dot \beta_t e^{\beta_t}\left\langle\nabla f(X_t), x  - X_{t} - \frac{1}{\dot \beta_t} \dot X_{s} \right\rangle  \, \label{eq:apply-dynamics}   \\
& =   \dot \beta_t e^{\beta_t} \langle \nabla f(X_t), x - X_t \rangle\, - e^{ \beta_t} \langle \nabla f(X_t), \dot X_t\rangle dt\, \notag   \\
& = \dot \beta_t e^{\beta_t} \langle \nabla f(X_t), x - X_t \rangle\,  -   \frac{d}{dt}\left(e^{ \beta_t} f(X_t)\right)\,   + \dot \beta_t e^{\beta_t}  f(X_t)  \notag\\
&= \dot \beta_t e^{\beta_t} [f(X_t)+ \langle \nabla f(X_t), x - X_t \rangle] -\frac{d}{dt}\left(e^{ \beta_t} f(X_t)\right)\,  \notag  \\ %\label{Eq:Line9}\\
& \leq  \dot \beta_t e^{\beta_t}f(x) - \frac{d}{dt}\left(e^{ \beta_t} f(X_t)\right)\, \label{eq:apply-convexity}\\
& = - \frac{d}{dt} \left(e^{\beta_t}\left(f(X_t) - f(x)\right)\right) \label{Eq:cond1}.
\end{align}
\end{subequations}
%\end{small}
Here~\eqref{eq:apply-dynamics} uses the momentum dynamics%~\eqref{Eq:EL}.
~\eqref{Eq:ELH} and~\eqref{Eq:ELZ}. 
The inequality~\eqref{eq:apply-convexity} follows from the convexity of $f$. Rearranging terms and taking $x = x^\ast$, we have shown that the function~\eqref{Eq:E} has nonpositive derivative for all $t$ and is hence a Lyapunov function for the family of momentum dynamics~\eqref{Eq:EL}.

 The structure of this derivation provides a tool for the analysis of several discretizations of momentum dynamics~\eqref{Eq:ELBreg}.
 In the following subsections, we demonstrate how to analyze the quasi-monotone subgradient method~\cite{Nesterov15}, the class of accelerated gradient methods~\cite{Baes09,Acceleration}, and the general family of conditional gradient algorithms~\cite{NesterovCond15} using the Lyapunov argument above.
 \subsection{Implicit and explicit Euler methods}
The methods that we study are based on compositions of Euler discretizations of the ODE $ \dot X_t = v(X_t)$.  Recall that the \emph{explicit Euler method} evaluates the vector field at the current point to determine a discrete-time step:
\begin{align*}
\frac{x_{k+1} - x_k}{\delta} = \frac{X_{t+ \delta} - X_t}{\delta}= v(X_t) = v(x_k),
\end{align*}
%This can be written,
%\begin{align*}
%x_{k+1} - x_k = \delta v(x_k).
%\end{align*}
whereas the \emph{implicit Euler method} evaluates the vector field at the future point:
\begin{align*}
\frac{x_{k+1} - x_k}{\delta} = \frac{X_{t+ \delta} - X_t}{\delta}= v(X_{t+\delta}) = v(x_{k+1}).
\end{align*}
%and  can be written
%\begin{align*}
%x_{k+1} - x_k = \delta v(x_{k+1}).
%\end{align*}
In our case, the momentum dynamics~\eqref{Eq:EL} are a system of two first-order equations, and one can compose the implicit and explicit Euler methods in four different ways, leading to four separate algorithms. To illustrate how this works, we write~\eqref{Eq:EL} as the following system of first-order ODEs,
 \begin{subequations}\label{Eq:DynaBeta}
 \begin{align}
 Z_t &= X_t + \frac{e^{\beta_t}}{\frac{d}{dt} e^{\beta_t}} \dot X_t, \label{Eq:Z}\\
 \frac{d}{dt} \nabla h(Z_t) &= -\left(\frac{d}{dt}e^{\beta_t}\right) \nabla f(X_t). \label{Eq:X}
 \end{align}
 \end{subequations}
 Taking $e^{\beta_t} = A_k$, $\dot X_t = \frac{x_{k+1} - x_k}{\delta}$, and $\frac{d}{dt}e^{\beta_t} =\frac{A_{k+1} - A_k}{\delta}$, the implicit Euler method applied to \eqref{Eq:Z} results in the following sequence:
 \begin{align*}
 z_{k+1} &=  x_{k+1} + \frac{A_k}{A_{k+1} - A_k} (x_{k+1} - x_k).
 \end{align*}
The explicit Euler method, on the other hand, results in the sequence 
\begin{align*}
 z_{k} &=  x_{k} + \frac{A_k}{A_{k+1} - A_k} (x_{k+1} - x_k).
\end{align*}
 For equation~\eqref{Eq:X}, using the identification $\frac{d}{dt} \nabla h(Z_t) = \frac{1}{\delta}(\nabla h(z_{k+1}) - \nabla h(z_k))$, the implicit Euler method results in the following sequence
 \begin{align*}
\nabla h(z_{k+1}) - \nabla h(z_k) = (A_{k+1} - A_k) \nabla f(x_{k+1}),
 \end{align*}
 whereas the explicit Euler method results in the sequence
 \begin{align*}
\nabla h(z_{k+1}) - \nabla h(z_k) = (A_{k+1} - A_k) \nabla f(x_{k}).
 \end{align*}
 In the following, we analyze three combinations of the implicit and explicit Euler methods using our Lyapunov framework. Furthermore, we show there are at least two slightly different methods, which do not comport to a straightforward discretization technique, that can also be analyzed using our Lyapunov analysis. 
   \subsection{Momentum-based algorithms}
 We give a short analysis of the result of implicit Euler discretization of both~\eqref{Eq:Z} and~\eqref{Eq:X}. We can write the algorithm as follows:
\begin{algorithm}[H]
%\begin{subequations}\label{Eq:AlgoForward}
\caption{Implicit Euler Based Method}
{\bf Assumptions:} $f, h$ are convex and differentiable.\\
Choose $A_0 = 1$, $x_{-1}= x_0 = z_0$ and $\tau_{k} = \frac{A_{k+1} - A_k}{A_k}$. Define recursively,
\begin{subequations}\label{Eq:AlgoForward1}
\begin{align}
z_{k} &= x_{k} + \frac{1}{\tau_{k-1}} (x_{k} - x_{k-1}),\\%\label{Eq:ZSeq}\\
x_{k+1} &= \underset{\substack{\,\,\,\,\,\,x\in\X\\ \,\,\,\,\,\,z =  \frac{1 + \tau_{k}}{\tau_{k}} x - \frac{1}{\tau_{k}} x_k}}{\text{arg\,\,\,min}} \left\{ f(x) + \frac{1}{A_{k+1} - A_k}D_h\left(z, z_k\right)\right\},
\end{align}
\end{subequations}
\end{algorithm}
\noindent the optimality conditions for which constitute our discretization
\begin{subequations}\label{Eq:ImplicEul}
\begin{align}
z_{k+1} = x_{k+1} + \frac{A_k}{A_{k+1} -A_{k}} (x_{k+1} - x_k),\label{Eq:ZSeq}\\
\nabla h(z_{k+1}) - \nabla h(z_k) = -(A_{k+1} - A_{k})\nabla f(x_{k+1})\label{Eq:XSeq}.
\end{align}
\end{subequations}
\noindent To analyze this algorithm, we make the same continuous to discrete time identifications as in~\eqref{Eq:E}, which results in the following discrete-time energy function:
\begin{align}\label{Eq:DiscLyapFunc1}
 E_k = A_k(f(x_k) - f(x)) + D_h(x, z_{k}). 
 \end{align}
\begin{theorem}
Consider the 
 Euler discretization~\eqref{Eq:ImplicEul}. Using the discrete Lyapunov function~\eqref{Eq:DiscLyapFunc1}, the following bound holds:
\begin{equation*}
E_{k+1} - E_k \leq \varepsilon_{k+1},
\end{equation*}
where the error is negative,
\begin{equation*}
\varepsilon_{k+1} = -D_h(z_{k+1}, z_k).
\end{equation*}
\end{theorem}
\begin{proof}
In view of definition~\eqref{Eq:DiscLyapFunc1}, the following holds,
\begin{small}
\begin{subequations}\label{Eq:LyapDiscArg}
\begin{align}
E_{k+1} - E_k &= A_{k+1}(f(x_{k+1}) - f(x)) - A_k(f(x_k) - f(x)) -\langle \nabla h(z_{k+1}) - \nabla h(z_k), x - z_{k+1}\rangle - D_h(z_{k+1}, z_k)\notag\\
& \overset{\eqref{Eq:XSeq}}{=} A_{k+1}(f(x_{k+1}) - f(x)) - A_k(f(x_k) - f(x)) + (A_{k+1} - A_k) \langle \nabla f(x_{k+1}), x - z_{k+1}\rangle - D_h(z_{k+1}, z_k)\notag\\
& \overset{\eqref{Eq:ZSeq}}{=} A_{k+1}(f(x_{k+1}) - f(x)) - A_k(f(x_k) - f(x)) + (A_{k+1} - A_k) \langle \nabla f(x_{k+1}), x - x_{k+1}\rangle \notag\\
&\quad + A_k \langle \nabla f(x_{k+1}), x_{k+1} - x_k\rangle- D_h(z_{k+1}, z_k)\notag\\
%
&\leq A_{k+1}(f(x_{k+1}) - f(x)) - A_k(f(x_k) - f(x)) + (A_{k+1} - A_k)(f(x) - f(x_{k+1})) \notag\\
&\quad  + A_k (f(x_k) - f(x_{k+1}))- D_h(z_{k+1}, z_k)\notag \\
&=- D_h(z_{k+1}, z_k), \notag
\end{align}
\end{subequations}
\end{small}
where the inequality follows from the convexity of $f$. 
\end{proof}
%From \eqref{Eq:LyapDiscArg}, we have shown that $E_{k+1} - E_k \leq \varepsilon_k $, where the error $\varepsilon_k = -D_h(z_{k+1}, z_k) \leq 0 $ is negative. 
\noindent Taking $x = x^\ast$, and writing $E_k \leq E_0$ allows us to obtain a convergence rate guarantee analogous to~\eqref{eq:Guarantee1}:
\begin{equation}
f(x_k) - f(x^\ast) \leq \frac{ A_0 (f(x_0) - f(x^\ast)) + D_h(x^\ast, x_0) }{A_k}.
\end{equation} 
%which is equivalent to using the same discrete-time identifications. 
%
We remark that the subequations corresponding to~\eqref{Eq:AlgoForward1} are difficult to solve. For reference, the proximal minimization method~\cite{ChenTeboulle93}, 
\begin{equation}
x_{k+1} = \arg\min_{x\in\X} \left\{ f(x) + \frac{1}{A_{k+1} - A_k}D_h\left(x, x_k\right)\right\},
\end{equation}
involves applying the implicit Euler method to the mirror descent dynamics, and both yield a $O(1/A_k)$ convergence rate. 


\subsection{The Quasi-monotone subgradient method}
\label{Sec:QuasiMono}
The quasi-monotone subgradient method was developed by Nesterov~\cite{Nesterov15} as an alternative to the dual averaging method.  It also achieves the $O(1/\sqrt{k})$ lower bound for the class of methods with bounded subgradients. The advantage of the quasi-monotone method however is that a convergence-rate guarantee can be shown for the entire sequence of iterates instead of the average (or minimum) iterate, as is the case with the dual averaging method in the non-Euclidean setting. {\em We begin by studying the quasi-monotone method in the situation where we have full gradients, which we assume to be bounded} and show later how the analysis extends to the setting where we only have access to bounded subgradients. We also set the dual averaging term $\gamma_k \equiv 1$ for convenience and give an analysis of the full algorithm in Appendix~\ref{App:ProofQuasiGen}.  With these minor modifications, the algorithm can be understood as the explicit Euler method applied to~\eqref{Eq:Z} and the implicit Euler method applied to~\eqref{Eq:X} (where now we take $e^{\beta_t} = A_{k+1}$, while leaving our identification for $\frac{d}{dt} e^{\beta_t}$ unchanged):



\begin{algorithm}[H]
\caption{The Quasi-Monotone (Sub)gradient Method $\gamma_k \equiv 1$}
{\bf Assumptions:} $f, h$ are convex and differentiable. $h$ is strongly convex and $f$ has bounded gradients, $\sup_{x\in\X}\|\nabla f(x)\| = G <\infty$. \\
Let $A_0 = 1$, $x_0 = z_0$, $\tau_k = \frac{A_{k+1} - A_k}{A_{k+1}}$, $\alpha_k = A_{k} - A_{k-1}$ . Define recursively,
\begin{subequations}\label{Eq:QuasiSub1}
\begin{align}
x_{k+1} &= \tau_k z_k + (1 - \tau_k)x_{k} ,\label{Eq:ZSeqQuasi1}\\
z_k &= \arg \min_{z\in \X} \left\{ \sum_{i=1}^k \alpha_i \langle \nabla f(x_i), z\rangle + D_h(z, x_0)\right\} \label{Eq:XSeqQuasi1}.
\end{align} 
\end{subequations}
\end{algorithm}
\noindent This modified quasi-monotone subgradient method has optimality conditions
\begin{subequations}\label{Eq:QuasiSub}
\begin{align}
z_{k} &= x_{k} + \frac{A_{k+1}}{A_{k+1} -A_{k}} (x_{k+1} - x_k),\label{Eq:ZSeqQuasi}\\
\nabla h(z_{k+1}) -\nabla h(z_k) &= -(A_{k+1} - A_{k})\nabla f(x_{k+1})\label{Eq:XSeqQuasi}.
\end{align} 
\end{subequations}
%%%%%%%%%%%%%%%%%%%%%%% Estimate Sequences

The following result illustrates how the discrete Lyapunov function~\eqref{Eq:DiscLyapFunc1} can be used to analyze~\eqref{Eq:QuasiSub}:
\begin{theorem}
\label{Thm:Quasi}
Using the Lyapunov function~\eqref{Eq:DiscLyapFunc1},
we can show the quasi-monotone method~\eqref{Eq:QuasiSub} satisfies \[E_{k+1} - E_k \leq \,\,\varepsilon_{k+1},\] where the error scales in the following way,%\footnote{For reference, the error for subgradient descent scales in the same way (see Appendix~\ref{App:MirrorDesc})}
\[\varepsilon_{k+1} = \frac{(A_{k+1}- A_k)^2}{2}\|\nabla f(x_{k+1})\|^2 \leq \frac{(A_{k+1}- A_k)^2}{2}G^2.\]
\end{theorem}
\noindent Taking $x = x^\ast$ and summing over $k$ results in the following convergence rate:
\begin{align}\label{Eq:BoundQuasi}
f(x_k) - f(x^\ast) \leq \frac{A_0 (f(x_0) - f(x^\ast))+D_h(x^\ast , x_0)  + \frac{1}{2}\sum_{i=1}^{k}\varepsilon_i }{A_k}.
\end{align}
If we optimize the bound~\eqref{Eq:BoundQuasi} over $A_k$, we can obtain an $O(1/\sqrt{k})$ convergence rate guarantee by setting a time-horizon for the algorithm to be run, and choosing $A_{k+1} - A_{k} =\frac{D_h(x,x_0)}{G\sqrt{k+1}}.$ Without this step, we suffer an additional $\log k$ factor in the the numerator.\footnote{See~\cite[2]{Nesterov15} for more details on this history on subgradient methods as well as step-size analysis.}

 In the proof given in Appendix~\ref{App:ProofQuasi}, convexity is the only property of $f$ that is necessary to show~\eqref{Eq:BoundQuasi}. Thus, the proof can be extended to include subgradient steps instead of full gradient steps, where now the condition on $f$ is that its subgradients are  bounded. This recovers the result of Nesterov~\cite{Nesterov15} using the technique of estimate sequences. 

\subsection{Other discretizations}
We give examples of two other algorithms that are the result of discretizing the momentum dynamics~\eqref{Eq:DynaBeta}, and analyze them using the discretized Lyapunov function~\eqref{Eq:DiscLyapFunc1}. The proofs of these results can be found in the Appendix~\ref{App:OtherDiscret}. The first method is the result of applying the explicit Euler method to~\eqref{Eq:X} and the implicit Euler method to~\eqref{Eq:Z}:
\begin{algorithm}[H]
\caption{Method 1}
{\bf Assumptions:} $f$ is continuously differentiable, $\|\nabla^2 f\|\leq L$, and $\X$ is convex and compact. \\
Let $A_0 = 1$, $x_0 = z_0$, $\alpha_k = A_{k+1} - A_k$ and $\tau_k = \frac{A_{k+1}- A_k}{A_{k}}$. Define recursively,
\begin{subequations}\label{Eq:QuasiSub2}
\begin{align}
z_{k+1} &= \arg \min_{z \in \X} \left\{\langle \nabla f(x_k),z \rangle + \frac{1}{\alpha_k} D_h(z, z_k)\right\}\\
x_{k+1} &= \frac{\tau_k}{\tau_k - 1}z_{k+1} + \frac{1}{\tau_k - 1}  x_k.
\end{align} 
\end{subequations}
\end{algorithm}
\noindent This algorithm has optimality conditions,
\begin{subequations}
\begin{align}
z_{k+1} &= x_{k+1} - \frac{A_{k}}{A_{k+1} -A_{k}} (x_{k+1} - x_k),\label{Eq:ZSeqOther1}\\
\nabla h(z_{k+1}) -\nabla h(z_k) &= -(A_{k+1} - A_{k})\nabla f(x_{k}) \label{Eq:XSeqOther1}.
\end{align}
\end{subequations}
\noindent Choosing $A_k = k(k+1)/2$ results in an $O(1/k)$ convergence rate. We remark that  this algorithm is structurally very similar to the conditional gradient method (see~\ref{App:OtherDiscret} for details). The second method does not correspond to a straightforward discretization technique and is given by the following sequences:
\begin{algorithm}[H]
\caption{Method 2}
{\bf Assumptions:} $f, h$ are convex and differentiable. $h$ is strongly-convex and $f$ has bounded gradients, $\sup_{x\in\X}\|\nabla f(x)\| = G <\infty$. 
Let $A_0 = 1$, $x_0 = z_0$ and $\tau_k = \frac{A_{k+1}- A_k}{A_{k}}$. Define recursively,
\begin{subequations}\label{Eq:QuasiSub3}
\begin{align}
x_{k+1}  &= \frac{\tau_k}{\tau_k + 1} z_k + \frac{1}{\tau_k+1}x_k,\\%\label{Eq:ZSeqQuasi}\\
z_{k+1} &= \arg \min_{z \in \X} \left\{\langle \nabla f(x_{k+1}),z\rangle + \frac{1}{\alpha_k} D_h(z, z_k)\right\}.
\end{align} 
\end{subequations}
\end{algorithm}
\noindent This algorithm has optimality conditions,
\begin{subequations}\label{Eq:QuasiSub3}
\begin{align}
z_{k} &= x_{k+1} + \frac{A_{k}}{A_{k+1} -A_{k}} (x_{k+1} - x_k),\label{Eq:ZSeqMethod2}\\%\label{Eq:ZSeqQuasi}\\
\nabla h(z_{k+1}) -\nabla h(z_k) &= -(A_{k+1} - A_{k})\nabla f(x_{k+1}).\label{Eq:XSeqMethod2}
\end{align} 
\end{subequations}
Notice that \eqref{Eq:QuasiSub3} is very similar to the quasi-monotone subgradient method~\eqref{Eq:QuasiSub1}. In particular, under the same assumptions, it achieves a $O(1/\sqrt{k})$ convergence rate. Furthermore, like the quasi-monotone subgradient method, it can be extended to functions which have bounded subgradients instead of full gradients and an additional weighting term can be added.

%%%%%%%%%%%%%%%%%%%%%ACCELERATED GRAD DESCENT %%%%%%%%%%%%%%
\subsection{Accelerated mirror descent}
\label{Sec:AccGrad}
We present two variants of the accelerated mirror descent algorithm (which is the accelerated gradient descent algorithm in the setting where $h = \frac{1}{2}\|\cdot \|^2$). Neither of these correspond to a straightforward discretization of~\eqref{Eq:EL}---they both entail introducing an additional sequence $\{y_k\}$ to obtain a better bound on the error. The first is the version introduced by Baes~\cite{Baes09}:
\begin{algorithm}[H]
\begin{subequations}\label{Eq:AcceleratedGrad2}
\caption{Accelerated Mirror Descent (Weakly Convex Setting)}
{\bf Assumptions:} $f, h$ are convex and differentiable. $h$ is $1$-strongly convex and $f$ has smooth gradients  $\|\nabla^{2} f\|\leq L$\\
Choose $A_0 = 1$, $M>0$, $\tilde A_{k+1} = L^{-1} A_{k+1}$,  $\tau_k = \frac{\tilde A_{k+1}- \tilde A_k}{\tilde A_{k+1}}:= \frac{\alpha_k}{\tilde A_{k+1}}$ and $x_0 = z_0 = y_0$. Define recursively, 
\begin{align}
z_k &= \arg \min_{z\in \X} \left\{ \sum_{i=1}^k \alpha_i \langle \nabla f(y_i), z\rangle + D_h(z, z_0)\right\}\label{Eq:ZSeqAcc2}\\
y_k &= \arg \min_{x \in \X} \left \{ \langle \nabla f(x_k), x\rangle + \frac{ L}{4M}\|y - x_k\|^2\right\}\\\
x_{k+1} &= \tau_k z_k + (1- \tau_k)  y_k\label{Eq:XSeqAcc2}
\end{align}
\end{subequations}
\end{algorithm}
\noindent This algorithm satisfies the following optimality conditions:
\begin{subequations}\label{Eq:AccGrad3}
\begin{align}
z_{k} &= y_k + \frac{\tilde A_{k+1}}{\tilde A_{k+1} -\tilde A_{k}} (x_{k+1} - y_k),  \label{Eq:ZSeqAcc3}\\%\label{Eq:ZSeqAcc2}\\
\nabla h(z_{k+1}) - \nabla h(z_{k}) &= -(\tilde A_{k+1} -  \tilde A_{k})  \nabla f(y_{k+1})  \label{Eq:XSeqAcc3}\\ %\label{Eq:XSeqAcc}\\
\quad M \|L^{-1} \nabla f(y_{k})\|_\ast^{2} &\leq L^{-1}\langle\nabla f(y_{k}), x_{k} - y_{k}\rangle. \label{Eq:YSeqAcc3}
\end{align}
\end{subequations}
The advantage of this version of the accelerated mirror descent algorithm is that it can be extended to accelerate higher-order gradient methods (the details of which we give in Appendix~\ref{App:ProofAccGrad}). The second version, originally introduced by Nesterov~\cite{Nesterov05}, entails setting $M = \frac{1}{2}$ and computing the mirror descent update~\eqref{Eq:ZSeqAcc2} from the gradient at $x$:
\begin{equation*}
z_k = \arg \min_{z\in \X} \left\{ \sum_{i=1}^k \alpha_i \langle \nabla  f(x_i), z\rangle + D_h(z, z_0)\right\}.
\end{equation*}
The resulting algorithm has the following optimality conditions:
\begin{subequations}
\begin{align}
z_{k} &= y_k + \frac{\tilde A_{k+1}}{\tilde A_{k+1} -\tilde A_{k}} (x_{k+1} - y_k), \label{Eq:ZSeqAcc4}\\
\nabla h(z_{k+1}) - \nabla h(z_{k}) &= -(\tilde A_{k+1} -  \tilde A_{k})  \nabla f(x_{k+1})\label{Eq:XSeqAcc4}\\
\quad f(y_k) - f(x_k)  &\leq  -\frac{1}{2L} \| \nabla f(x_{k})\|_\ast^{2}. \label{Eq:YSeqAcc4}
\end{align}
\end{subequations}
As mentioned above, to obtain a convergence guarantee for both algorithms we have replaced the current state $x_k$ by another sequence $y_k$ which needs to satisfy~\eqref{Eq:YSeqAcc3}. Furthermore, we have replaced our sequence $A_{k+1}$ by $\tilde A_{k+1}$ which depends on the Lipschitz parameter. We make the same adjustments to the Lyapunov function~\eqref{Eq:DiscLyapFunc1} which we summarize in the following theorem:
\begin{theorem} \label{Thm:AccGrad}
Using the following Lyapunov function
\begin{equation}\label{Eq:LyapAccGrad}
E_k = \tilde A_k(  f(y_k) -  f(x)) + D_h(x, z_k),
\end{equation}
we can show
  % which has the effect of scaling the gradient by $\epsilon$ as well. 
\[E_{k+1} - E_k \leq \varepsilon_{k+1},\]
where the error scales in the following way 
\[\varepsilon_{k+1} =  \left(\frac{1}{2}(\tilde A_{k+1} -\tilde A_{k})^{2}- \tilde A_{k+1}M\right)\|\nabla f(y_{k+1})\|^2 \]
for the accelerated gradient method~\eqref{Eq:AcceleratedGrad2}.
\end{theorem}
We choose $x = x^\ast$ and $A_k$ such that $\tilde A_{k+1}^{-1}(\tilde A_{k+1} -\tilde A_{k})^{2} \leq 2M$ to ensure the error is nonpositive. With these choices, we obtain the following convergence rate,
\begin{equation}
 f(y_k) -  f(x^\ast) \leq \frac{\tilde A_0(f(x_0) - f(x^\ast)) + D_h(x^\ast, x_0)}{\tilde A_k}.
\end{equation}
The condition to ensure the error is nonpositive restricts the choice of sequence $\{A_k\}_{k=1}^\infty$ so that it can grow at most quadratically. This results in a $O(1/k^2)$ convergence rate.
We give a proof of both of these methods in Appendix~\ref{App:ProofAccGrad}.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%FRANK-WOLFE %%%%%%%%%%%%%%%%%
\subsection{The conditional gradient algorithm}
\label{Sec:CondGrad}
 The conditional gradient method, also called the \emph{Frank-Wolfe} algorithm~\cite{FrankWolfe}, has garnered renewed interest over the last several years. This interest has inspired several different analyses~\cite{Freund14, Bach15} of the algorithm. Of particular note is the analysis provided by Nesterov~\cite{NesterovCond15}, who used to technique of estimate sequences to evaluate the algorithm under varying smoothness assumptions and step-size conditions. Nesterov also extended the algorithm to provide an analysis of a trust-region Newton-type method. Here, we explore the conditional gradient method from a dynamical view, and give an alternate analysis of both methods. To do so, we first revisit the derivation of the Lyapunov function. Notice, that if $\dot \beta_t >0 $ and we can ensure the following inequality for our dynamical system
\begin{align}\label{Eq:Frank1} 
0 \leq \langle \nabla f(X_t), x-  Z_t\rangle,
\end{align}
 $\forall x \in \X $, where 
\begin{align}\label{Eq:Frank2}
Z_t = X_t + \frac{1}{\dot \beta_t} \dot X_t,
\end{align}
then the Lyapunov analysis~\eqref{Eq:LyapAnal} follows without the use of the Bregman divergence: 
\begin{subequations}\label{Eq:LyapAnalFrank}
\begin{align}
0 &\leq  
\dot \beta_t e^{\beta_t} \left\langle \nabla f(X_t), x -Z_t\right\rangle \,  \notag\\
& \overset{\eqref{Eq:LyapAnal}}{\leq}  \dot \beta_t e^{\beta_t} f(x) -\frac{d}{dt}\left( e^{ \beta_t}  f(X_t)\right)\, \notag\\
& = -\frac{d}{dt} \left( e^{\beta_t}\left(f(X_t) - f(x)\right)\right). \notag
\end{align}
\end{subequations}
Integrating shows that the following function 
\begin{equation}\label{Eq:FrankE}
\mathcal{E}_t = e^{\beta_t} (f(X_t) - f(x)).
\end{equation} 
is a Lyapunov function for any family of dynamics satisfying~\eqref{Eq:Frank1} and~\eqref{Eq:Frank2}.  We now turn our attention to the conditional gradient method, which maintains the following iterates: 
\begin{algorithm}[H]
\begin{subequations}\label{Eq:Fank-Wolfe}
\caption{Conditional Gradient Method}
{\bf Assumptions:} $f$ is continuously differentiable, $\|\nabla^2 f\|\leq L$, and $\X$ is convex and compact. \\
Choose $A_0 = 1$, $x_0 = z_0$ $\tau_k = \frac{A_{k+1} - A_k}{A_{k+1}}:= \frac{\alpha_{k+1}}{A_{k+1}}$.
\begin{align}
z_k &= \arg \min_{z \in \X}\,\,  \langle \nabla f(x_k), z\rangle, \label{Eq:XSeqFrank1}\\
x_{k+1} &= \tau_k z_k + (1- \tau_k) x_k.\label{Eq:ZSeqFrank1}
\end{align}
%\end{align}
\end{subequations}
\end{algorithm}
\noindent In particular, the conditional gradient method satisfies the following variational inequalities:
\begin{subequations}\label{eq:Frank-WolfeAlgo}
\begin{align}
0 \leq \langle \nabla f(x_k), x - z_k\rangle \quad \forall x \in \X,\\
z_{k} = x_k + \frac{A_{k+1}}{A_{k+1} -A_{k}} (x_{k+1} - x_k).\label{Eq:ZSeqFrank}
\end{align}
\end{subequations}
This suggests the following Lyapunov analysis of the conditional gradient method:
\begin{theorem}
Using the Lyapunov function
\begin{equation}\label{Eq:FrankWolfeLyap}
E_k = A_k(f(x_k) - f(x^\ast)),
\end{equation}
we can show that the conditional gradient method~\eqref{eq:Frank-WolfeAlgo} satisfies
\begin{equation}\label{Eq:CondGradBound}
E_{k+1} - E_k \leq \varepsilon_{k+1},
\end{equation}
where the error scales in the following way,
\[  \varepsilon_{k+1} = \frac{L}{2}\frac{(A_{k+1} - A_k)^2}{A_{k+1}} diam(\X)^2. \]
This matches the analysis of the conditional gradient method by Nesterov using the technique of estimate sequences as well as the analysis provided by others~\cite{Freund14, Bach15}. In particular, choosing $A_k = \frac{k(k+1)}{2}$ gives the standard $O(1/k)$ convergence rate (see Nesterov~\cite[Eq (2.16)]{NesterovCond15} for details). 
\end{theorem}
The proof of this theorem can be found in Appendix~\ref{App:CondMethod}.
%
%
%%%%%%%%%%%%%%EXTENSIONS
\subsubsection{Extensions}
We briefly point out that the algorithm resulting from applying an implicit Euler method to both \eqref{Eq:Frank1}  and \eqref{Eq:Frank2}, 
\begin{subequations}
\begin{align*}
z_{k+1} &= \arg \min_{z \in \X}\,\,  \langle \nabla f(x_{k+1}), z\rangle\\
z_{k+1} &= x_{k+1} + \frac{A_{k}}{A_{k+1} -A_{k}} (x_{k+1} - x_k),
\end{align*}
\end{subequations}
results in an algorithm for which we can show $E_{k+1} - E_k \leq 0$ using~\eqref{Eq:FrankWolfeLyap}. However, like most implicit discretization techniques applied to the momentum dynamics~\eqref{Eq:ELBreg}, this does not lead to an algorithm with tractable subproblems.
%
We now turn our attention to analyzing the trust region Newton-like method introduced by Nesterov. This method is given by the following update, 
\begin{algorithm}[H]
\begin{subequations}\label{Eq:Fank-Wolfe}
\caption{Conditional Gradient Method~\cite[(5.1)]{NesterovCond15}}
{\bf Assumptions:} $f$ is twice continuously differentiable, $\|\nabla^3 f\|\leq L$, and $\X$ is convex and compact. \\
Choose $A_0 = 1$, $x_0 = z_0$ $\tau_k = \frac{A_{k+1} - A_k}{A_{k+1}}$.
\begin{align}
x_{k+1} \in \arg \underset{y=(1-\tau_k)x_k + \tau_k x}{\min} \left\{\langle \nabla f(x_k), y- x_k\rangle + \frac{1}{2} \langle \nabla^2 f(x_k)(y-x_k), y-x_k\rangle
: x \in dom(\X) \right\}
\end{align}
%\end{align}
\end{subequations}
\end{algorithm}
This algorithm has the following optimality conditions: 
\begin{subequations}\label{Eq:FrankTrust}
\begin{align}
z_{k} &= x_{k} + \frac{A_{k+1}}{A_{k+1} -A_{k}} (x_{k+1} - x_k),\label{Eq:ZSeqFrankTrust}\\
\langle \nabla f(x_k)& + \nabla^2 f(x_k)(x_{k+1}- x_k), y- x_{k+1}\rangle\geq 0, \label{Eq:Var2Cond}\\
\forall y &= (1-\tau_k)x_k + \tau_k x, \quad x \in dom(\X).\label{Eq:Var3Cond}
\end{align}
\end{subequations}
In Appendix~\ref{App:CondMethod} we show how to recover the bound shown by Nesterov using the Lyapunov function~\eqref{Eq:FrankWolfeLyap}. 
%
%
%
%%%%%%%%%%STRONG CONVEXITY
\section{Strong Convexity}
In this section, we introduce a new family of momentum dynamics which can be discretized to obtain the accelerated gradient descent algorithm in the setting where the objective function $f$ is strongly convex. In Appendix~\ref{App:StrongConvexity}, we show how to derive a Lyapunov function using the momentum dynamics. Furthermore, we show how to discretize the dynamics to obtain a discrete-time algorithm which can be analyzed by performing an analogous discretization of the Lyapunov function. 
\label{Sec:StrongConv}
\subsection{Lyapunov analysis}
In this section, we study the Lyapunov function~\eqref{eq:general-sc-lyap} in the Euclidean setting,
\begin{equation}\label{Eq:StrongLyap}
\E_t = e^{\beta_t}\left(f(X_t) - f(x^\ast) + \frac{\mu}{2}\|x^\ast- Z_t \|^2\right),
\end{equation}
and use it to analyze the following dynamics: 
\begin{subequations}\label{Eq:StrongDyn}
\begin{align}
Z_t &= X_t + \frac{1}{\dot \beta_t} \dot X_t\\
\mu \dot Z_t &= - \mu\dot X_t - \dot \beta_t \nabla f(X_t).
\end{align}
\end{subequations}
Note that~\eqref{Eq:StrongDyn} is the Euler-Lagrange equation for the following Lagrangian:
\begin{equation}\label{Eq:LagStrongConv}
\mathcal{L}(x, \dot x, t) = e^{\gamma_t + \beta_t + \alpha_t}  \left(\frac{\mu}{2}e^{-2\alpha_t}\left\|\dot x\right\|^2 - f(x)\right),
\end{equation}
under the same ideal scaling conditions~\eqref{Eq:IdeSca}, where we take $e^{\alpha_t} = \dot \beta_t$. Furthermore, this Lagrangian can be generalized to include non-Euclidean geometries; however, further exploration of this Lagrangian is outside the scope of this work. Here, we demonstrate that~\eqref{Eq:StrongLyap} can be used to establish an $O(e^{-\beta_t})$ rate of convergence for~\eqref{Eq:StrongDyn} with a strong convexity assumption. To do so, note that if we can ensure  $\dot \E_t = e^{\beta_t}\dot \beta_t \tilde{\E}_t + e^{\beta_t} \dot{\tilde{\E}}_t \leq 0$, which amounts to ensuring
$\dot{\tilde{\E}}_t \leq - \dot \beta_t \tilde{\E}_t$
for 
\begin{equation}\label{Eq:StrongLyap2}
\tilde \E_t = f(X_t) - f(x^\ast) + \frac{\mu}{2}\|x^\ast - Z_t \|^2,
\end{equation}
then \eqref{Eq:StrongLyap} is a Lyapunov function. To that end, we have the following argument:
\begin{subequations}\label{Eq:StrongContProof}
\begin{align}
\dot {\tilde{\E}}_t  &= \langle \nabla f(X_t), \dot X_t\rangle - \mu\left\langle \dot Z_t, x^\ast - X_t -\frac{1}{\dot \beta_t}  \dot X_t\right\rangle \notag\\
& = \langle \nabla f(X_t), \dot X_t\rangle +  \left\langle\mu \dot X_t +  \dot \beta_t \nabla f(X_t),x^\ast - X_t -\frac{1}{\dot \beta_t}  \dot X_t\right\rangle \notag\\
& = \dot \beta_t \langle  \nabla f(X_t),x^\ast - X_t\rangle +    \mu\left\langle  \dot X_t, x^\ast - X_t- \frac{1}{\dot \beta_t}  \dot X_t\right\rangle \notag\\
& \leq - \dot \beta_t \left(f(X_t) - f(x^\ast) + \frac{\mu}{2}\|x^\ast - X_t\|^2\right)+   \mu \left\langle  \dot X_t, x^\ast - X_t- \frac{1}{\dot \beta_t}  \dot X_t\right\rangle\label{Eq:StrongContStrong}\\
& =  - \dot \beta_t \left(f(X_t) - f(x^\ast) + \frac{\mu}{2}\left\|x^\ast - X_t - \frac{1}{\dot \beta_t} \dot X_t\right\|^2\right) -\mu\left\langle \dot X_t, x^\ast - X_t - \frac{1}{\dot \beta_t} \dot X_t\right\rangle- \frac{\mu}{2\dot \beta_t}\|\dot X_t\|^2\notag \\
&\quad + \mu \left\langle \dot X_t,x^\ast - X_t- \frac{1}{\dot \beta_t}  \dot X_t\right\rangle\notag\\
& \leq   - \dot \beta_t \left(f(X_t) - f(x^\ast) + \frac{\mu}{2}\left\|x^\ast - Z_t\right\|^2\right) \notag,
\end{align}
\end{subequations}
where \eqref{Eq:StrongContStrong} uses our strong convexity assumption. In the following subsections, we demonstrate how to discretize the dynamic~\eqref{Eq:StrongDyn}, and how this discretization can be analyzed using a Lyapunov argument analogous to \eqref{Eq:StrongContProof}.
%
%
%
%
\subsection{Discretizing the dynamics}
As a proof of concept, we begin by analyzing an implicit discretization of the dynamics~\eqref{Eq:StrongDyn}. 
Denoting  $\tau_k = \frac{A_{k+1} - A_k}{A_k}$, $\dot Z_t = \frac{z_{k+1} - z_k}{\delta}$, $\dot X_t = \frac{x_{k+1} - x_k}{\delta}$, $\frac{1}{\dot \beta_t} = \frac{e^{\beta_t}}{\frac{d}{dt} e^{\beta_t}} = \frac{A_{k}}{\frac{A_{k+1} - A_k}{\delta}}$, 
we obtain the following algorithm:
\begin{algorithm}[H]
\caption{Implicit Euler Based Method (Strong Convexity)}
{\bf Assumptions:} $f, h$ are convex and differentiable.\\
Choose $A_0 = 1$, $x_{-1}=x_0 = z_0$ and $\tau_{k} = \frac{A_{k+1} - A_k}{A_{k+1}}$. Define recursively,
\begin{subequations}
\begin{align}\label{Eq:AlgoForward}
z_{k} &= x_{k} + \frac{1}{\tau_{k-1}}(x_{k} - x_{k-1})\\
x_{k+1} &= \arg\underset{x\in\X}{\text{min}} \left\{ f(x) - \frac{\mu}{2\tau_k}\left\|\tau_k(x_k - z_k) - (x - x_k)\right\|^2\right\},
\end{align}
\end{subequations}
\end{algorithm}
\noindent The optimality conditions of this algorithm corresponds to our discretization: 
\begin{subequations}\label{Eq:ImpStrong}
\begin{align}
z_{k+1} - z_k &= \tau_k \left( x_{k+1} - z_{k+1} - \frac{1}{\mu} \nabla f(x_{k+1})\right) \label{Eq:ZImpStrong}\\
z_{k+1} &= x_{k+1} + \frac{1}{\tau_k}(x_{k+1} - x_k)\label{Eq:ImpStrong}.
\end{align}
\end{subequations}
\begin{theorem}
The following bound
\begin{equation*}
\tilde E_{k+1} -\tilde E_k \leq - \tau_k \tilde E_{k+1} + \varepsilon_{k+1},
\end{equation*}
can be shown for \eqref{Eq:ImpStrong} using the discrete-time Lyapunov function
\begin{equation}
\tilde E_k = f(x_k) - f^\ast + \frac{\mu}{2}\|x^\ast- z_k\|^2,
\end{equation}
where the error is negative,
\begin{equation*}
\varepsilon_{k+1} = - \frac{\mu}{2} \|z_{k+1} - z_k\|^2.
\end{equation*}
\end{theorem}
\begin{proof}
Notice that a similar argument to~\eqref{Eq:StrongContProof} holds:
\begin{subequations}
\begin{align*}
\tilde E_{k+1} -\tilde  E_k & =  f(x_{k+1}) - f(x_k) - \mu\langle z_{k+1} - z_k, x^\ast - z_{k+1}\rangle - \frac{\mu}{2} \|z_{k+1} - z_k\|^2 \\
&\overset{\eqref{Eq:ZImpStrong}}{=}  f(x_{k+1}) - f(x_k)+  \tau_k \langle  \nabla f(x_{k+1}) -\mu(x_{k+1} - z_{k+1}), x^\ast - z_{k+1}\rangle  - \frac{\mu}{2} \|z_{k+1} - z_k\|^2\\
&=   \tau_k \langle  \nabla f(x_{k+1}), x^\ast - x_{k+1}\rangle  + f(x_{k+1}) - f(x_k) + \tau_k \langle  \nabla f(x_{k+1}),x_{k+1} - z_{k+1}\rangle - \frac{\mu}{2} \|z_{k+1} - z_k\|^2\\
&\quad+\mu \tau_k\langle z_{k+1} - x_{k+1}, x^\ast - z_{k+1}\rangle\\
&\leq    -\tau_k\left (f(x_k) - f(x^\ast) + \frac{\mu}{2}\|x^\ast - x_{k+1}\|^2\right)   +\tau_k \langle  \nabla f(x_{k+1}),x_{k+1} - z_{k+1}\rangle   \\
&\quad-\frac{\mu}{2} \|z_{k+1} - z_k\|^2 + \mu \tau_k\langle z_{k+1} - x_{k+1}, x^\ast - z_{k+1}\rangle \\
& \leq - \tau_k \left( f(x_{k+1}) - f(x^\ast) + \frac{\mu}{2}\|x^\ast - z_{k+1}\|^2\right) + f(x_{k+1}) - f(x_k) + \tau_k \langle \nabla f(x_{k+1}), x_{k+1} - z_{k+1}\rangle\\
&\quad  - \frac{\mu}{2} \|z_{k+1} - z_k\|^2. \\
& \overset{\eqref{Eq:ImpStrong}}{=}   - \tau_k \left( f(x_{k+1}) - f(x^\ast) + \frac{\mu}{2}\|x^\ast - z_{k+1}\|^2\right)+  f(x_{k+1}) - f(x_k)+\langle \nabla f(x_{k+1}), x_k - x_{k+1}\rangle\\
&\quad  - \frac{\mu}{2} \|z_{k+1} - z_k\|^2. \\
& \leq  - \tau_k \left( f(x_{k+1}) - f(x^\ast) + \frac{\mu}{2}\|x^\ast - z_{k+1}\|^2\right) - \frac{\mu}{2} \|z_{k+1} - z_k\|^2. 
\end{align*}
\end{subequations}
Therefore,
\begin{equation}
\tilde E_{k+1} -\tilde  E_k \leq - \tau_k \left( f(x_{k+1}) - f(x^\ast) + \frac{\mu}{2}\|x^\ast - z_{k+1}\|^2\right) = -\tau_k \tilde E_{k+1}.
\end{equation}
\end{proof}
\noindent Choosing $\tau_k = \sqrt{\kappa}$ gives the $O(e^{-\sqrt{\kappa} k})$ convergence rate. However, there is no restriction on $\tau_k$;  we are free to scale it arbitrarily. Given the subproblems for this algorithm are difficult to solve, we consider other discretizations based on the explicit Euler method. In particular, we utilize the same trick of introducing a new sequence $\{y_k\}_{k=1}^\infty$, as in accelerated gradient descent~\eqref{Eq:AccGrad3},  in order to bound the error. We find that we can use a Lyapunov argument to analyze the following two sequences: 
\begin{subequations}\label{Eq:StrongConv1}
\begin{align} 
z_{k+1} - z_k &= \tau_k \left( x_{k+1} - z_k - \frac{1}{\mu} \nabla f(x_{k+1})\right), \label{Eq:ZSeq2}\\
\tau_k(x_{k+1} - z_k) &= y_k - x_{k+1} \label{Eq:Coupling2}\\
y_{k+1} &= x_{k+1} - \frac{1}{L} \nabla f(x_{k+1}),\label{Eq:Grad2}
\end{align}
\end{subequations}
and 
\begin{subequations}\label{Eq:StrongConv2}
\begin{align}
z_{k+1} - z_k &= \tau_k \left( x_k - z_k - \frac{1}{\mu} \nabla f(x_k)\right), \label{Eq:ZSeq1}\\
\tau_k(x_k - z_k) &= y_k - x_k\label{Eq:Coupling1}\\
y_{k+1} &= x_k - \frac{1}{L} \nabla f(x_k). \label{Eq:Grad1}
\end{align}
\end{subequations}
The second sequence~\eqref{Eq:StrongConv2} is the accelerated gradient scheme introduced by Nesterov~\cite[(2.2.8)]{Nesterov04}, which can be further simplified into two sequences. We summarize our results in the following theorem:
\begin{theorem}Using the following Lyapunov function, 
\begin{equation}\label{Eq:LyapStrong}
\tilde E_k = f(y_k) - f(x^\ast) + \frac{\mu}{2}\|z_k - x^\ast\|^2,
\end{equation}
we can show 
\begin{equation}
\tilde E_{k+1} -\tilde  E_k \leq -\tau_k \tilde E_k + \varepsilon_{k+1}
\end{equation}
for both methods~\eqref{Eq:StrongConv1} and~\eqref{Eq:StrongConv2}, where
\begin{equation}
\varepsilon_{k+1} = \left(\frac{\tau_k^2}{2\mu} - \frac{1}{2L}\right)\|\nabla f(x_{k+1})\|^2 + \left(\frac{\tau_kL}{2}-\frac{\mu}{2\tau_k}\right)\|x_{k+1} - y_k\|^2
\end{equation}
 and 
\begin{equation}
\varepsilon_{k+1} = \left(\frac{\tau_k^2}{2\mu} - \frac{1}{2L}\right)\|\nabla f(x_k)\|^2 + \left(\frac{\tau_kL}{2}-\frac{\mu}{2\tau_k}\right)\|x_k - y_k\|^2,
\end{equation}
for~\eqref{Eq:StrongConv1} and~\eqref{Eq:StrongConv2} respectively.
\end{theorem}
The proof of this result is in Appendix \ref{App:StrongConvexity}. In both cases, to ensure the error is nonpositive we must set  $\tau_k\leq 1/\sqrt{\kappa}$, where $\kappa = L/\mu$ is the condition number.  Indeed, setting $\tau_k = 1/\sqrt{\kappa}$ results in the recurrence
\[
\tilde E_{k+1} \leq \left(1-\tfrac{1}{\sqrt{\kappa}}\right) \tilde  E_k, 
\]
which ensures the standard convergence rate for Nesterov's method applied to strongly convex functions~\cite{Nesterov04}.\footnote{This bound was also shown in the recent work of Karimi~\cite{Karimi16}.}

\section{Estimate Sequences}

In this section, we connect our Lyapunov framework directly to the technique of estimate sequences.  We derive continuous-time estimate sequences directly from our Lyapunov function and demonstrate how these two techniques are equivalent in continuous and discrete time.

We begin with a brief review of the technique of estimate sequences. 
In \cite{Nesterov04}, Nesterov introduced estimate sequences by giving the following definition:
\begin{definition}\cite[2.2.1]{Nesterov04} A pair of sequences $\{\phi_k(x)\}_{k=1}^\infty$ and $\{A_k\}_{k=0}^\infty$ $A_k \geq1$ is called an {\em estimate sequence} of function $f(x)$ if 
\[ \frac{1}{A_k} \rightarrow 0 \]
and for any $x \in \R^n$ and all $k \geq 0$, we have 
\begin{equation}\label{Eq:Ineq1}
\phi_k(x) \leq \Big(1 - \frac{1}{A_k}\Big)f(x) + \frac{1}{A_k}\phi_0(x).
\end{equation}
\end{definition}
\noindent The following lemma, given by Nesterov, explains why estimate sequences are useful.
%
%
%Lemma 
\begin{lemma}\label{Lem:Nest2.2.1} \cite[2.2.1]{Nesterov04}
If for some sequence $\{x_k\}_{k\geq0}$ we have 
\begin{equation}\label{Eq:Seq}
f(x_k) \leq \phi_k^\ast \equiv \min_{x \in \X} \phi_k(x),
\end{equation}
then $f(x_k) - f(x^\ast) \leq \frac{1}{A_k} [\phi_0(x^\ast) - f(x^\ast)]$. 
\end{lemma}
%
%
\begin{proof}The proof is straightforward:
\begin{align*}
f(x_k) \overset{\eqref{Eq:Seq}}{\leq} \phi_k^\ast \equiv \min_{x \in \X} \phi_k(x) &\overset{\eqref{Eq:Ineq1}}{\leq}\min_{x \in\X}\left[\Big(1 - \frac{1}{A_k}\Big)f(x) + \frac{1}{A_k}\phi_0(x)\right]\\
&\leq \Big(1 - \frac{1}{A_k}\Big)f(x^\ast) + \frac{1}{A_k}\phi_0(x^\ast).
\end{align*}
Rearranging gives the desired inequality.
\end{proof}
%
%
Notice that this definition is not constructive and the definition of an estimation sequence is rather general. Finding sequences which satisfy these conditions is a non-trivial task. The next proposition, formalized by Baes in \cite{Baes09} as an  extension of Nesterov's Lemma 2.2.2 \cite{Nesterov04}, provides guidance for constructing estimate sequences. This construction is used in \cite{Nesterov04, Nesterov05, Nesterov08, Baes09, Nesterov15, NesterovCond15}, and is the only known way so far to construct an estimate sequence.  We will see below that this particular class of estimate sequences can be turned into our Lyapunov functions with a few algebraic manipulations (and vice-versa).
\begin{proposition}\cite[2.2]{Baes09}\label{Eq:Prop}
 Let $\phi_0: \X \rightarrow \R$ be a convex function such that $\min_{x \in \X} \phi_0(x)\geq f^\ast$. Suppose also that we have a sequence $\{f_k\}_{k\geq 0}$ of functions from $\X$ to $\R$ that underestimates $f$:
\begin{align}\label{Eq:Under}
f_k(x) \leq f(x) \quad \text{ for all $x \in \X$ and all $k \geq 0$}.
\end{align}
Define recursively $A_0 = 1$ 
\begin{align}
\alpha_{k} &:= A_{k+1} - A_k \label{Eq:alpha}\\ 
\tau_k &:= \frac{a_{k}}{A_{k+1}},\label{Eq:tau}
\end{align}
and 
\begin{equation}\label{Eq:Est}
\phi_{k+1}(x) := (1 - \tau_k) \phi_k(x) + \tau_k f_k(x) = \frac{1}{A_{k+1}} \left(A_0\phi_0(x) + \sum_{i=0}^k a_i f_i(x)\right),
\end{equation}
for all $k\geq 0$. Then $\left(\{\phi_k\}_{k\geq0}, \{A_k\}_{k\geq0}\right)$ is an estimate sequence.
\end{proposition}


\noindent From \eqref{Eq:Seq} and \eqref{Eq:Est}, we observe that the following  invariant:
\begin{align}\label{Eq:EstSeqInv}
A_{k+1} f(x_{k+1}) &\leq \min_x A_{k+1} \phi_{k+1}(x) = \min_x  \sum_{i=0}^k \alpha_i f_i(x) +  A_0\phi_0(x),
\end{align}
is maintained. In \cite{Nesterov15, NesterovCond15}, this technique was extended to incorporate a error term $\{\tilde \varepsilon_k\}_{k=1}^\infty$,
\begin{equation}\label{Eq:Est2}
\phi_{k+1}(x) - \frac{\tilde \varepsilon_{k+1}}{A_{k+1}} := (1 - \tau_k) \left(\phi_k(x) - \frac{\tilde \varepsilon_{k}}{A_k}\right) + \tau_k f_k(x) = \frac{1}{A_{k+1}} \left(A_0(\phi_0(x) - \tilde \varepsilon_0) + \sum_{i=0}^k a_i f_i(x)\right) 
\end{equation}
Rearranging, we have
\begin{align}\label{Eq:EstSeqInv2}
A_{k+1} f(x_{k+1}) &\leq \min_x A_{k+1}\phi_{k+1}(x) = \min_x  \sum_{i=0}^k \alpha_i f_i(x) +  A_0\left(\phi_0(x) - \frac{\tilde \varepsilon_0}{A_0}\right) + \tilde \varepsilon_{k+1}.
\end{align}
Notice that the analogous argument to that of Lemma~\ref{Lem:Nest2.2.1} holds:
\begin{subequations}\label{Eq:EstSeqErr}
\begin{align}
A_{k+1} f(x_{k+1}) &\leq \sum_{i=0}^k \alpha_i f_i(x^\ast) + A_0 (\phi_0(x^\ast) - \tilde \varepsilon_0)  + \tilde \varepsilon_{k+1}\notag\\
&\overset{\eqref{Eq:Under}}{\leq} \sum_{i=0}^k \alpha_i f(x^\ast) +  A_0\phi_0(x^\ast)  + \tilde \varepsilon_{k+1}\notag\\
& \overset{\eqref{Eq:alpha}}{=} A_{k+1} f(x^\ast) +  A_0\phi_0(x^\ast) + \tilde \varepsilon_{k+1},
\end{align}
\end{subequations}
where in the second inequality we use the fact that $\varepsilon_k \geq 0,\,\, \forall k$. Rearranging,
\begin{align*}
f(x_{k+1}) - f(x^\ast) \leq \frac{1}{A_{k+1}}\left(A_0\phi_0(x^\ast)  + \tilde \varepsilon_{k+1}\right).
\end{align*}
Thus, we simply need to choose our sequence $\{A_k\}_{k=1}^\infty$ to ensure $\tilde \varepsilon_{k+1}/ A_{k+1} \rightarrow 0$. The following table illustrates the choices of $\phi_k(x)$ and $\tilde \varepsilon_k$ for the four methods discussed earlier: 
\begin{center}
\begin{table}[h!]
\begin{tabular}{|c|c|c|c|}%\label{Eq:Table}
\hline 
Algorithm & $f_i(x)$ &  $\phi_k(x)$ & $\tilde \varepsilon_{k+1}$ \\ \hline
$\substack{\text{Quasi-Monotone} \\\text{Subgradient}\\\text{ Method}}$ & linear & $\frac{1}{A_k}D_h(x, z_k) + f(x_k)$ & $\frac{1}{2}\sum_{i=1}^{k+1} \frac{(A_i - A_{i-1})^2}{2}G^2$  \\ \hline
$\substack{\text{Accelerated } \\\text{Gradient Method} \\\text{ (Weakly Convex)}} $& linear  &$\frac{1}{A_k}D_h(x, z_k) +  f(x_k) $ & 0 \\  \hline
$\substack{\text{Accelerated  } \\\text{Gradient Method}\\\text{ (Strongly Convex)}} $  & quadratic &$  f(x_k)  + \frac{\mu}{2}\|x - z_k\|^2$ &  0 \\ \hline
$\substack{\text{Conditional Gradient}\\\text{ Method }}$ & linear  & $f(x_k)$ & $ \frac{L}{2}\sum_{i=1}^{k+1}\frac {(A_{i} - A_{i-1})^2}{A_{i}} diam(\X)^2$\\
\hline
\end{tabular}
\caption{Choices of estimate sequences for various algorithms}
\label{table:Table}
\end{table}
\end{center}
Here ``linear'' is defined as 
\begin{equation}
f_i(x) = f(x_i) + \langle \nabla f(x_i), x - x_i\rangle
\end{equation}
and ``quadratic'' is defined as
\begin{equation}
f_i(x) = f(x_i) + \langle \nabla f(x_i), x - x_i\rangle + \frac{\mu}{2}\|x - x_i\|^2.
\end{equation}

\subsection{Equivalence between Estimate Sequences and Lyapunov Functions}
We now demonstrate an equivalence between these two frameworks.  The continuous-time view shows that the errors in both the Lyapunov function and estimate sequences are due to discretization errors. We provide a sketch for how this works for most of these methods, leaving some of the details for Appendix~\ref{App:EstSeq}.
\subsubsection{Accelerated Gradient Descent}
\paragraph{Equivalence in discrete time}
The discrete-time estimate sequence~\eqref{Eq:Est} for accelerated gradient descent can be written:
\begin{align*}
\phi_{k+1}(x) &:= f(x_{k+1}) + \frac{1}{A_{k+1}}D_h(x, z_{k+1}) \\
& \overset{\eqref{Eq:Est}}{=} (1 - \tau_k) \phi_k(x)  + \tau_k f_k(x) \\
&\overset{\text{Table}~\ref{table:Table}}{=} \left(1 - \frac{\alpha_k}{A_{k+1}} \right)\left(f(x_k) +\frac{1}{A_k} D_h(x, z_k)\right) + \frac{\alpha_k}{A_{k+1}} f_k(x).
\end{align*}
Multiplying through by $A_{k+1}$, we have
\begin{align*}
A_{k+1} \left(f(x_{k+1}) + \frac{1}{A_{k+1}}D_h(x, z_{k+1})\right) &= (A_{k+1} - \alpha_k) \left(f(x_k) + \frac{1}{A_k}D_h(x, z_k)\right) + \alpha_k f_k(x)\\
&= (A_{k+1} - (A_{k+1} - A_k)) \left(f(x_k) + \frac{1}{A_k}D_h(x, z_k)\right) + (A_{k+1} - A_k) f_k(x)\\
&=  A_k \left(f(x_k) + \frac{1}{A_k}D_h(x, z_k) \right) + (A_{k+1} - A_k) f_k(x)\\
& \overset{\eqref{Eq:Under}}{\leq}  A_k f(x_k) + D_h(x, z_k)+ (A_{k+1} - A_k) f(x).
\end{align*}
Rearranging, we obtain the inequality $E_{k+1} \leq E_k$ for our Lyapunov function~\eqref{Eq:DiscLyapFunc1}. 
%
%
%
Going the other direction, from our Lyapunov analysis we can derive the following bound:
\begin{subequations}\label{Eq:EstSeqErr}
\begin{align}
E_k &\leq E_0\\
A_{k}(f(x_{k}) - f(x)) + D_h(x, z_{k}) &\leq A_0 (f(x_0) - f(x)) + D_h(x, z_0) \notag\\
A_{k}\left(f(x_{k}) - \frac{1}{A_{k}}D_h(x, z_{k})\right) &\leq (A_k -  A_0) f(x) + A_0\left(f(x_0)+ \frac{1}{A_0}  D_h(x^\ast, z_0)\right) \notag \\
A_k \phi_k(x) &\leq (A_k -A_0) f(x) + A_0 \phi_0(x). \label{Eq:EstSeq}
\end{align}
\end{subequations}
Rearranging, we obtain the estimate sequence~\eqref{Eq:Ineq1}, with $A_0 = 1$:
\begin{subequations}
\begin{align}
\phi_k(x) &\leq \Big(1 -\frac{A_0}{A_k}\Big) f(x) + \frac{A_0}{A_k} \phi_0(x) \\
&= \Big(1 - \frac{1}{A_k}\Big)f(x) + \frac{1}{A_k}\phi_0(x).
\end{align}
\end{subequations}
\paragraph{Equivalence in continuous time}
From the derivation of the Lyapunov function \eqref{Eq:LyapAnal}, we have the following equality:
\begin{align*}
\frac{d}{ds}D_h\left(x, Z_s\right)  &= \dot \beta_t e^{\beta_t}  [f(X_s)+ \langle \nabla f(X_s), x - X_s \rangle] -  \frac{d}{ds}\left( e^{ \beta_s}  f(X_s)\right)\,.
\end{align*}
If we integrate and assume we start from rest $\dot X_0 = 0$, we can write this as,
\begin{align}\label{Eq:Ineq2}
 D_h(x, Z_t) \leq \int_0^t\dot \beta_t e^{\beta_t} [f(X_s)+ \langle \nabla f(X_s), x - X_s \rangle]ds  -  e^{ \beta_t}  f(X_t) + e^{\beta_0} f(X_0) + D_h(x, X_0).
\end{align}
Now, by pattern matching, we can use this inequality to extract a continuous-time estimate sequence. 
From~\eqref{Eq:Ineq2} we have the inequality,
\begin{align*}
e^{\beta_t}f(X_t) + D_h(x, Z_t) &\leq \int_0^t\dot \beta_t e^{\beta_t}  [f(X_s)+ \langle \nabla f(X_s), x - X_s \rangle]ds + e^{\beta_0}f(X_0)+  D_h(x,Z_0)\\
&\leq \int_0^t \dot \beta_t e^{\beta_t}f(x) + e^{\beta_0}f(X_0)+  D_h(x,Z_0)\\
 &= (e^{\beta_t} - e^{\beta_0}) f(x) + e^{\beta_0}f(X_0)+  D_h(x,Z_0).
\end{align*}
Comparing this to~\eqref{Eq:EstSeq}, if we define
\begin{align*}
\phi_t(x)=f(X_t) + e^{-\beta_t}D_h(x, Z_t)
\end{align*}
then the above discussion shows that $\{\phi_t(x), e^{\beta_t}\}$ is a continuous-time estimate sequence. In Appendix~\ref{App:EstSeq} we extend this argument to the quasi-monotone subgradient method by adding an error term $\varepsilon_k$. 

%
%
%
\subsubsection{Conditional gradient method} 
\paragraph{Continuous-time estimate sequence}
For the conditional gradient method, the following condition needs to hold:
\begin{align}\label{Eq:InEq4}
0 &\leq \int_0^t\dot \beta_t e^{\beta_t}  [f(X_s)+ \langle \nabla f(X_s), x - X_s \rangle] -  \frac{d}{ds}\left( e^{ \beta_s}  f(X_s)\right)\,ds.
\end{align}
 We can write this as
\begin{align*}
e^{ \beta_t}  f(X_t) \leq \int_0^t\dot \beta_t e^{\beta_t}  [f(X_s)+ \langle \nabla f(X_s), x - X_s \rangle]ds   + e^{\beta_0} f(X_0),
\end{align*}
and, from the derivation of the Lypaynov function, we conclude
\begin{align*}
e^{\beta_t} f(X_t) \leq  (e^{\beta_t} - e^{\beta_0}) f(x) + e^{\beta_0}f(X_0).
\end{align*}
Thus, $\{f(X_t),e^{\beta_t}\}$ is a continuous-time estimate sequence for this method. In Appendix~\ref{App:EstSeq}, we show the corresponding equivalence in discrete time.
%

%
%
\subsection{Accelerated gradient strong convexity}
Notice that for the dynamics in~\eqref{Eq:StrongDyn}, the following holds:
\begin{align}\label{Eq:Strong}
 \frac{d}{dt}\left(e^{\beta_t}\frac{\mu}{2} \|x - Z_t\|^2\right)\, &=  \dot \beta_t e^{\beta_t} \frac{\mu}{2} \|x - Z_t\|^2 - e^{\beta_t} \mu\langle \dot Z_t, x - Z_t\rangle \notag\\
 &=  \dot \beta_t e^{\beta_t} \frac{\mu}{2} \|x - Z_t\|^2 + e^{\beta_t} \mu\langle \dot X_t, x - Z_t\rangle  + \dot  \beta_t e^{\beta_t} \left\langle \nabla f(X_t), x - X_t - \frac{1}{\dot \beta_t} \dot X_t\right\rangle \notag \\
  &=\dot  \beta_t e^{\beta_t} \frac{\mu}{2} \|x - Z_t\|^2 + \dot \beta_te^{\beta_t} \mu\langle  Z_t - X_t, x - X_t \rangle - \dot \beta_te^{\beta_t} \frac{\mu}{2}\| Z_t - X_t\|^2 \notag\\
 &\quad + \dot \beta_t e^{\beta_t} \langle \nabla f(X_t), x - X_t \rangle  - e^{\beta_t} \langle \nabla f(X_t),\dot X_t\rangle \notag\\
 & =  \dot \beta_t e^{\beta_t} \frac{\mu}{2} \|x - X_t\|^2 + \dot \beta_t e^{\beta_t} \langle \nabla f(X_t), x - X_t \rangle   - e^{\beta_t} \langle \nabla f(X_t),\dot X_t\rangle \notag \\
 &=   \dot \beta_t e^{\beta_t}\left( f(X_t) + \langle \nabla f(X_t), x - X_t \rangle+ \frac{\mu}{2} \|x - X_t\|^2\right)   -  \frac{d}{dt}\left( e^{ \beta_t}  f(X_t)\right),
\end{align}
where the the second to last line follows from completing the square.\footnote{We remark also that this derivation also shows how to derive the Lyapunov function~\eqref{Eq:StrongLyap} from the dynamics in~\eqref{Eq:StrongDyn}.}
 Integrating both sides results in the following inequality, 
\begin{align*}
e^{\beta_t} f(X_t) + e^{\beta_t}\frac{\mu}{2} \|x - Z_t\|^2 &\leq \int_0^t\dot \beta_t e^{\beta_t}[ f(X_s)+ \langle \nabla f(X_s), x - X_s \rangle+ \frac{\mu}{2} \|x - X_s\|^2]\, ds\\
& \quad+ e^{\beta_0}f(X_0) + e^{\beta_0}\frac{\mu}{2} \|x - X_0\|^2.\\
&\leq \int_0^t\dot \beta_t e^{\beta_t}f(x) \,ds+ e^{\beta_0}f(X_0) + e^{\beta_0}\frac{\mu}{2} \|x - X_0\|^2.
\end{align*}
Therefore, 
\begin{align*}
e^{\beta_t} \left(f(X_t) +\frac{\mu}{2} \|x - Z_t\|^2\right) \leq (e^{\beta_t} - e^{\beta_0}) f(x) + e^{\beta_0}  \left(f(X_0) + \frac{\mu}{2} \|x - Z_0\|^2\right),
\end{align*}
and hence \begin{align*}
\{f(X_t) + \frac{\mu}{2} \|x - Z_t\|^2, e^{\beta_t}\}
\end{align*}
is a continuous-time estimate sequence. In Appendix~\ref{App:EstSeq}, we show the equivalence in discrete time between estimate sequences and our Lyapunov framework in this setting.

\section{Discussion}
In this paper, we have presented a Lyapunov framework for analyzing a variety of gradient-based methods for optimization. We showed that a single family of Lyapunov functions can be used to verify the convergence rates of a large collection of momentum methods, making Polyak's original physical intuition rigorous.  We demonstrated that convergence rates could be understood as the consequence of discretization errors incurred when passing from continuous to discrete time.  Consistently, implicit discretization schemes result in harder subproblems for algorithms, but provide an almost exact approximation of the continuous-time dynamics.   On the other hand, when explicit discretization schemes are used, the algorithms incur discretization error, and require assumptions about the problem instance to guarantee good convergence properities.

We close with a discussion of some possible directions for future work.  First, note that requiring that the continuous-time Lyapunov function remain a Lyapunov function in discrete time places significant constraints on which ODE solvers can be used.  In this paper, we show that we can derive new algorithms using a restricted set of ODE techniques (several of which are nonstandard) but it remains to be seen if other methods can be applied in this setting.  Techniques such as the midpoint method and Runge Kutta provide more accurate solutions of ODEs than Euler methods~\cite{Butcher20001}.  Is it possible to analyze such techniques as optimization methods?  We expect that these methods do not achieve better asymptotic convergence rates, but may inherit additional favorable properties.  Determining the advantages of such schemes could provide more robust optimization techniques in certain scenarios.

Several restart schemes have been suggested for the strongly convex setting based on the momentum dynamics~\eqref{Eq:ELBreg}. In many settings,  while the Lipschitz parameter can be estimated using backtracking line-search, the strong convexity parameter is often hard---if not impossible---to estimate~\cite{SuBoydCandes15}.  Therefore, many authors~\cite{ODonoghue15,SuBoydCandes15,Krichene15} have developed heuristics to empirically speed up the convergence rate of the ODE (or discrete-time algorithm), based on model misspecification. In particular, both Su, Boyd, and Candes~\cite{SuBoydCandes15} and Krichene, Bayen and Bartlett~\cite{Krichene15} develop restart schemes designed for the strongly convex setting based on the momentum dynamics~\eqref{Eq:ELBreg}. Our analysis suggests that restart schemes based on the dynamics~\eqref{Eq:StrongDyn} might lead to better results. 

As mentioned in Section~\ref{Sec:StrongConv}, the dynamics~\eqref{Eq:StrongDyn} developed for the setting when $f$ is strongly convex can be viewed variationally, as the Euler-Lagrange equation for a different Lagrangian functional. While outside the scope of this paper, the Lagrangian~\eqref{Eq:LagStrongConv} can be generalized to a non-Euclidean setting. In follow-up work, we plan to study this second Lagrangian family and its properties further. %A natural question is whether the 
Other extensions of the Lyapunov analysis left for followup work are to nondeterministic settings such as accelerated coordinate ascent and stochastic gradient descent. A natural question we hope to answer is whether a dynamical perspective can be fruitfully applied in these settings. 

Earlier work by Drori and Teboulle~\cite{DroriTeboulle13}, Kim and Fessler~\cite{Kim2016}, Taylor \emph{et al}~\cite{Taylor2016}, and Lessard~\emph{et al}~\cite{Lessard14} have shown that optimization algorithms could be analyzed by solving convex programming problems.  In particular, Lessard~\emph{et al} show that Lyapunov-like potential functions called \emph{integral quadratic constraints} can be found by solving a constant-sized semidefinite programming problem. It would be interesting to see if these results can be adapted to directly search for Lyapunov functions like those studied in this paper.  This would provide a method to automate the analysis of new techniques, possibly moving beyond momentum methods to novel families of optimization techniques.

\section*{Acknowledgements}
We would like to give special thanks to Andre Wibisono for the many helpful discussion involving this paper. We would also like to thank Orianna Demassi for her very useful feedback and Stephen Tu who caught a small error in a previous version of this paper.   ACW was supported by an
NSF Graduate Research Fellowship. This work was supported in part by the Mathematical Data Science program of the Office of Naval Research under grant number N00014-15-1-2670.
\bibliographystyle{plain}
\bibliography{Draft14.bib}

\clearpage
\appendix
\section*{Appendix}
We present the analyses for several results presented in the main text. The Appendix is organized as follows: %Appendix~\ref{App:Mirror} presents a review of mirror descent from a dynamical perspective. In particular,  a Lyapunov analysis is derived similar to~\eqref{Eq:LyapAnal}. This part of the presentation is based on~\cite{NemirovskiiYudin}. 
Appendix~\ref{App:Momentum} presents the proofs for all the algorithms in Section~\ref{Sec:MomLyap} using our Lyapunov framework and Appendix~\ref{App:StrongConvexity} presents results for the strong convexity setting. Finally, Appendix~\ref{App:EstSeq} presents results showing the equivalence between estimate sequences, as formulated by Baes~\cite{Baes09} and our Lyapunov framework. 
% \section{Mirror Descent Dynamics}
% \label{App:Mirror}
% We begin with a smooth manifold $\X$ with a local metric $\mathrm{g}(x)$ and a function $f:\X \rightarrow \R$ we would like to minimize. The gradient flow associated with $f$ is the flow induced by the differential equation,
%\begin{equation}
%\dot X_t = v(X_t),
%\end{equation} where the vector field $v(X_t)$ is the ``steepest descent'' direction (the direction that makes $f$ decrease the fastest):
%\begin{equation}
%v(X_t) = \arg \min _v \left\{ \langle \nabla f(X_t), v\rangle + \frac{1}{2}\|v\|_{\mathrm{g}(X_t)}^2\right\}.
%\end{equation}
%We can write the gradient flow equation explicitly as,
% \begin{equation}\label{Eq:ManGradFlow}
%\dot X_t = - \mathrm{g}(X_t)^{-1} \nabla f(X_t).
%\end{equation}
%
%The dynamics which forms the basis for the mirror descent algorithm arises from a special structure that appears when the metric is chosen to be the Hessian of a strictly convex function $h$:
%\begin{equation}\label{Eq:NatGrad}
%\dot X_t = -[\nabla^2 h(X_t)]^{-1} \nabla f(X_t).
%\end{equation}
%which we can also write as,
%\begin{equation}\label{Eq:MirrorFlow}
%\frac{d}{dt} \nabla h(X_t) = - \nabla f(X_t).
%\end{equation}
%The ability to rewrite \eqref{Eq:NatGrad} as \eqref{Eq:MirrorFlow} is a manifestation of the following fact: \begin{center}{\em When $h:\X \rightarrow \R$ is strictly convex and $\mathrm{g}(x)= \nabla^2 h(x)$, there is a special map \\$\phi:\X \rightarrow \mathcal{Y}$ called the {\em mirror map}, given by $\phi = \nabla h$, whose push-forward maps\\ a gradient flow on $\X$ to a gradient flow on $\mathcal{Y}$. \cite{Acceleration}}\end{center} It can be checked that the following holds under the mirror map:
%
%\begin{center}
%\begin{tikzpicture}
%\node  at (-4.8,3.6) {$\X$};
%\filldraw[fill=blue!6,opacity=0.4]
% (-4.8,2) ellipse (3cm and 1.2cm);
%
%\node at (0, 4) {{\bf ``Mirror Map''}};
%\node at (0, 3.5) {$y = \nabla h(x)$};
%\node at (-4.8,-.3) {\small Hessian Metric $\nabla^2 h(x)$};
%\node at (-4.8,.3) {\small Function $f(x)$ };
%\node at (-4.8,-.9) {\small Gradient Flow $\dot X_t = -[\nabla^2 h(X_t)]^{-1}\nabla f(X_t) $};
%
%\node  at (4.8,3.6) {$\mathcal{Y}$};
%\node at (4.8,-.3) {\small Hessian Metric $\nabla^2 h^\ast(y) = [\nabla h^2(x)]^{-1}$};
%\node at (4.8,.3) {\small Function $f(\nabla h^\ast(y)) = \tilde{f}(y)$};
%\node at (4.8,-.9) {\small Gradient Flow $\dot Y_t = -[\nabla^2 h^\ast(Y_t)]^{-1}\nabla \tilde{f}(Y_t)$};
%\node at (5.65,-1.5) {\small $ = -\nabla f(\nabla h^\ast(Y_t)) $};
%\filldraw[fill=blue!6,opacity=0.4]
% (4.8,2) ellipse (3cm and 1.2cm);
% \path[->] (-3,2.4) edge [bend left=30] (3,2.4);
%
%
%
%
%\end{tikzpicture}
%\end{center}
%
%
%\paragraph{Time-dilation} 
%The gradient flow equation does not explicitly depend on time. As in \cite{Acceleration}, let  $\tau: \mathbb{T} \rightarrow \mathbb{T}'$ be a smooth twice-continuously differential function, where $\mathbb{T}' = \tau(\mathbb{T}) \subseteq \R$ is the image of $\mathbb{T}$. Given a curve $X:\mathbb{T}' \rightarrow \X$, we consider the reparameterized curve $Y:\mathbb{T} \rightarrow \X$ given by 
%\[Y_t = X_{\tau(t)}.\]
% That is,  the new curve is obtained by traversing the old curve at a new speed. If we consider the arbitrary time-dilation function $\tau(t) = e^{\beta(t)}:= e^{\beta_t}$, where $\dot \beta_t >0$, applied to the mirror descent dynamics \eqref{Eq:MirrorFlow}, we obtain the following equation, %that the time-dilated mirror descent dynamic is given by the equation
%\begin{equation}\label{Eq:MirrorTime}
%\frac{d}{dt} \nabla h(X_t) = -\dot \tau(t)\nabla f(X_t) = - \dot \beta_t e^{\beta_t} \nabla f(X_t).
%\end{equation}
%Since the vector field explicitly depends on time, \eqref{Eq:MirrorTime} is no longer a gradient flow. Nevertheless, a we can generate a Lyapunov function from the general time-dilated dynamics. 
%%we will outline in the following sections. 
%\subsection{Lyapunov Analysis}
%Using the Bregman divergence of $h$,
%\begin{equation}
%D_h(y,x) = h(y) - h(x) - \langle \nabla h(x), y- x\rangle, 
%\end{equation}
%we illustrate how the special property of the Hessian metric provides a Lyapunov analysis of the mirror descent dynamic~\eqref{Eq:MirrorFlow}. First, notice that 
%\begin{subequations}\label{Eq:Disc}
%\begin{align}
%\frac{d}{dt} D_h(x^\ast,X_t) & =  \frac{d}{dt} \left\{h(x^\ast) - h(X_t) - \langle \nabla h(X_t), x^\ast- X_t\rangle \right\} ds\notag\\
%& = - \langle \nabla h(X_t), \dot X_t\rangle -\left\langle \frac{d}{dt} \nabla h(X_t),x^\ast - X_t\right\rangle  + \langle \nabla h(X_t), \dot X_t\rangle \notag\\
%&=  -\left\langle \frac{d}{dt} \nabla h(X_t),x^\ast - X_t\right\rangle\notag\\
%& \overset{\eqref{Eq:MirrorTime}}{=}  \dot \beta_t e^{\beta_t} \left\langle \nabla f(X_t), x^\ast - X_t\right\rangle \notag\\
%& \leq  \dot \beta_t e^{\beta_t} (f(X_t) - f(x^\ast)) dt,\label{Eq:Bound1} 
%\end{align}
%\end{subequations}
%where in the last step we have used the convexity of $f$. Jensen's inequality ensures a $O(e^{-\beta_t})$ convergence rate on the average iterate,  %using Jensen's, we obtain a bound on the time-averaged iterate $\hat X_s = \frac{\int_0^t X_s ds}{t}$, 
%%we obtain the following bound
%\begin{align*}
%f\left(\frac{\int_0^t \dot \beta_s e^{\beta_s} X_s ds}{e^{\beta_t} - e^{\beta_0}}\right) - f(x^\ast) &\leq \frac{ - \int_0^t \frac{d}{ds} D_h(x^\ast,X_s)ds}{e^{\beta_t} - e^{\beta_0}} \\
%&\leq \frac{D_h(x^\ast,X_0) - D_h(x^\ast, X_t)}{e^{\beta_t} - e^{\beta_0}}\\
%&\leq \frac{D_h(x^\ast,X_0)}{e^{\beta_t} - e^{\beta_0}}.
%\end{align*}
% However, notice that the primal form~\eqref{Eq:NatGrad} of the mirror descent dynamics allows us to obtain a stronger guarantee,
% \begin{subequations}\label{Eq:Lem}
% \begin{align}
%\frac{d}{dt}\left( e^{\beta_t} f(X_t) \right) &= \dot \beta_t e^{\beta_t} f(X_t) + e^{\beta_t}\langle \nabla f(X_t), \dot X_t\rangle\notag \\
%&= \dot \beta_t e^{\beta_t}f(X_t) - \dot \beta_t e^{2\beta_t}\langle \nabla f(X_t), \nabla^2 h(X_t)^{-1}\nabla f(X_t)\rangle \notag\\
%& = \dot \beta_t e^{\beta_t}f(X_t) - \dot \beta_t e^{2\beta_t}\frac{1}{2}\|\nabla f(X_t)\|_{\ast,X_t}^2 \notag \\
%&\leq \dot \beta_t e^{\beta_t}f(X_t). 
%\end{align}
%\end{subequations}
%%Therefore 
%%\[- f(X_t) \leq -\frac{d}{dt} \left\{t f(X_t)\right\} \]
%If we plug this into \eqref{Eq:Bound1}, we obtain the following inequality,
%%Of course, this analysis 
%\begin{align*}
% \frac{d}{dt} \left( e^{\beta_t}( f(X_t) - f(x^\ast))\right) \leq\dot \beta_t e^{\beta_t} \left( f(X_t) - f(x^\ast)\right) \leq - \frac{d}{dt} D_h(x^\ast,X_t). 
%\end{align*}
%Integrating gives a Lyapunov function for the mirror descent dynamics~\eqref{Eq:MirrorFlow}.
%
%%
  %%%%%%%%%%%%%%%%%
    \section{Momentum Algorithms}
        \label{App:Momentum}
\subsection{Proof of Theorem~\ref{Thm:Quasi}}
\label{App:ProofQuasi}
%We use the Lyapunov function 
%\begin{equation*}
%E_k = A_k (f(x_k) - f(x^\ast)) + D_h(x^\ast, z_k)
%\end{equation*}
%to obtain a convergence guarantee for the quasi-monotone subgradient method~\eqref{Eq:QuasiSub}.
Denoting $D^\ast_{k+1, k} = D_h(x^\ast,z_{k+1})- D_h(x^\ast,z_{k})$, observe that
%\label{App:ProofQuasi}
%\begin{small}
\begin{subequations}
\begin{align}
D^\ast_{k+1, k}  &= -D_h(z_{k+1}, z_k) +  \langle \nabla h(z_{k+1}) - \nabla h(z_k), z_{k+1} - x^\ast\rangle \label{Eq:Mirror}\\
&\overset{\eqref{Eq:XSeqQuasi}}{\leq} -\frac{1}{2}\|z_k - z_{k+1}\|^2 -(A_{k+1} -A_{k}) \langle \nabla f(x_{k+1}), z_{k+1} - x^\ast\rangle \label{Eq:StrongConvAssump1} \\
&= -\frac{1}{2}\|z_k - z_{k+1}\|^2 - (A_{k+1} -A_{k}) \langle \nabla f(x_{k+1}), z_{k+1} - z_k\rangle -(A_{k+1} -A_{k}) \langle \nabla f(x_{k+1}), z_k - x^\ast\rangle \notag\\
&\leq\frac{(A_{k+1} -A_{k})^2}{2} \|\nabla f(x_{k+1})\|_\ast^2 - (A_{k+1} -A_{k}) \langle \nabla f(x_{k+1}), z_k - x^\ast\rangle \label{Eq:HoldBound1}\\
& \overset{\eqref{Eq:ZSeqQuasi}}{=}\frac{(A_{k+1} -A_{k})^2}{2} \|\nabla f(x_{k+1})\|_\ast^2 - (A_{k+1} - A_k) \left\langle \nabla f(x_{k+1}), \frac{A_{k+1}}{(A_{k+1} -A_{k})}(x_{k+1} - x_k) + x_k - x^\ast\right\rangle \notag \\
& =\frac{(A_{k+1} -A_{k})^2}{2} \|\nabla f(x_{k+1})\|_\ast^2- A_{k+1} \left\langle \nabla f(x_{k+1}), x_{k+1} - x_k\right\rangle - (A_{k+1} -A_{k})\langle \nabla f(x_{k+1}), x_{k+1} - x^\ast\rangle \notag\\
&\quad + (A_{k+1} -A_{k})\langle \nabla f(x_{k+1}), x_{k+1} - x_k \rangle \notag\\
& = \frac{(A_{k+1} -A_{k})^2}{2} \|\nabla f(x_{k+1})\|_\ast^2- A_{k} \langle \nabla f(x_{k+1}), x_{k+1} - x_k\rangle  - (A_{k+1} -A_{k})\langle \nabla f(x_{k+1}), x_{k+1} - x^\ast\rangle. \notag\\
&\leq A_k(f(x_{k}) - f(x_{k+1})) - (A_{k+1} -A_{k})(f(x_{k+1}) - f(x^\ast)) + \frac{(A_{k+1} -A_{k})^2}{2} \|\nabla f(x_{k+1})\|_\ast^2 \notag\\
& = A_k (f(x_k) - f(x^\ast)) - A_{k+1}(f(x_{k+1}) - f(x^\ast)) + \frac{(A_{k+1} -A_{k})^2}{2} \|\nabla f(x_{k+1})\|_\ast^2, \notag
\end{align}
\end{subequations}
%\end{small}
where \eqref{Eq:StrongConvAssump1} follows from the strong convexity assumption on $h$, and \eqref{Eq:HoldBound1} uses the Fenchel-Young inequality. Thus, we obtain the following bound,
\begin{subequations}
\begin{align*}
E_{k+1} - E_k &\leq  \frac{(A_{k+1} -A_{k})^2}{2} \|\nabla f(x_{k+1})\|_\ast^2. 
\end{align*}
for the Lyapunov function \eqref{Eq:DiscLyapFunc1}.
\end{subequations}



%%%%%%%%%%%%%%%%
\subsection{Proof of General Quasi-Monotone Method}
\label{App:ProofQuasiGen}
In \ref{App:ProofQuasi} we have set  the dual averaging term $\gamma_k \equiv 1$  for simplicity.
%\noindent The proof of Theorem~\ref{Thm:Quasi} can be found in Appendix.
%Notice, we have set the dual averaging term $\gamma_k \equiv 1$. This is for simplicity of the presentation. 
In its more general form, we add an increasing sequence $\{\gamma_k\}_{k=1}^\infty$ to the algorithm, and~\eqref{Eq:XSeqQuasi1} can be written as, 
\begin{equation*}
z_k = \arg \min_{z\in \X} \left\{ \sum_{i=1}^k \alpha_i \langle \nabla f(x_i), z\rangle + \gamma_kD_h(z, z_0)\right\} \label{Eq:XSeqQuasi2},
\end{equation*}
and which satisfies the optimality condition,
\begin{align}\label{Eq:DualAvgX}
\gamma_{k+1} \nabla h(z_{k+1})  - \gamma_k \nabla h(z_k)  =-(A_{k+1} - A_{k}) \nabla f(x_{k+1}).
\end{align}
For this method, the notion of a ``prox-center'' is essential. We provide an analysis of  this algorithm using the discrete Lyapunov function 
\begin{equation}\label{Eq:Lyap3}
E_k = A_k(f(x_k) - f(x^\ast)) + \gamma_k (D_h(x^\ast, z_k) -D_h(x^\ast, x_0)).
\end{equation}
First, however, we analyze the dynamics and show how to obtain the discrete Lyapunov function~\eqref{Eq:Lyap3} for the dual-averaged algorithm.
 
 \paragraph{The dynamics} We analyze the momentum dynamics~\eqref{Eq:ELBreg} with an additional increasing weighting term ($\dot \gamma_t \geq 0$), which we require to be positive, $\gamma_t\geq 0$, $\forall t\in \R$:
 \begin{subequations}\label{Eq:DualAvgDyn}
\begin{align}
\frac{d}{dt}\Big(\gamma_t \nabla h(Z_t)\Big) &= -\dot \beta_t e^{\beta_t}\nabla f(X_t)\\
Z_t &= X_t + \frac{1}{\beta_t} \dot X_t.
\end{align}
\end{subequations}
With this additional weighting term we obtain the following bound, 
\begin{subequations}\label{Eq:Last}
  \begin{align}
 \frac{d}{dt} \Big( \gamma_t D_h(x^\ast,Z_t) \Big) \, dt
&=\dot \gamma_t [h(x^\ast) - h(Z_t)] - \dot \gamma_t \langle\nabla h(Z_t), x - Z_t \rangle-\gamma_t\left \langle \frac{d}{dt} \nabla h(Z_t), x - Z_t\right \rangle \notag\\
&= \dot \gamma_t [h(x^\ast) - h(Z_t)] - \left \langle \frac{d}{dt} \Big(\gamma_t  \nabla h(Z_t)\Big), x^\ast - Z_t \right\rangle \notag\\
&  \overset{\eqref{Eq:DualAvgDyn}}{=} \dot \gamma_t [h(x^\ast) - h(Z_t)]  +  \dot \beta_t e^{\beta_t} \left\langle \nabla f(X_t), x^\ast - X_t - \frac{1}{\dot \beta_t} \dot X_t\right\rangle \notag\\
& \overset{\eqref{Eq:LyapAnal}}{\leq} \dot \gamma_t [h(x^\ast) - h(Z_t)]   - \frac{d}{dt}\left( e^{\beta_t}\left(f(X_t) - f(x^\ast)\right)\right).
%& \leq  \int_0^t \dot \gamma_s D_h(x^\ast, Z_0)  -\int_0^t \frac{d}{ds}\left\{ e^{\beta_s}\left(f(X_s) - f(x^\ast)\right)\right\} ds,
\end{align}
\end{subequations}
\paragraph{Prox-center}  We now require $\X$ to be a compact set,  and  define $X_0$ to be the ``prox-center'' of $\X$:
\begin{align*}
X_0 = \arg\min_{x \in \X} \,h(x).
\end{align*}
Without loss of generality, we choose $h(X_0) = 0$, so that $h(x) =D_h(x,X_0) \geq 0, \, \forall x \in \X$.
Using this definition and rearranging~\eqref{Eq:Last}, we obtain the following bound:
\begin{align*}
 \frac{d}{dt}\left( e^{\beta_t}\left(f(X_t) - f(x^\ast)\right)\right) \,\leq  \frac{d}{dt} \Big( \gamma_t D_h(x^\ast, Z_0) -\gamma_t D_h(x^\ast,Z_t) \Big) ,
\end{align*}
from which we can conclude that the following function, 
\begin{equation}
\E_t = e^{\beta_t}( f(X_t) - f(x^\ast)) + \gamma_t (D_h(x^\ast, Z_t) - D_h(x^\ast, Z_0)),
\end{equation}
is a Lyapunov function. We also obtain the following convergence rate guarantee, 
\begin{equation}\label{Eq:DualAvgBound}
f(X_t) - f(x^\ast) \leq \frac{\gamma_t D_h(x^\ast,Z_0)}{e^{\beta_t}}.
\end{equation}

\paragraph{The algorithm} 
Using the following equality, 
\begin{align}\label{InEq:DualAvgBound}
\gamma_{k+1} D_h(x^\ast, z_{k+1}) - \gamma_{k} D_h(x^\ast, z_k)& =\gamma_{k} D_h(z_{k+1}, z_k)  +  \langle \gamma_{k+1} \nabla h(z_{k+1}) - \gamma_{k} \nabla h(z_k), z_{k+1} - x^\ast \rangle \notag \\&\quad + (\gamma_{k+1} - \gamma_{k})(h(x^\ast) - h(z_{k+1})),
\end{align}
%
%
 we can analyze what happens when use the additional weighting term~\eqref{Eq:DualAvgX} in the quasi-monotone method, instead of~\eqref{Eq:XSeqQuasi}. We define $x_0 \in \X$ to be the {\em prox-center} over the set that is being optimized over. In discrete time, this amounts to the analogous condition that $h(x) \geq D_h(x ,x_0) \geq  \frac{\sigma}{2}\|x_0 - x\|^2, \forall x \in \X$, where we used the $\sigma$-strong convexity assumption on $h$ in the last inequality. For simplicity, we rescale $h$ so that $\sigma =1$.\footnote{Note, this is equivalent to choosing $h(x_0)= 0 $ and $x_0$ to be the minimizer of $h$ ($\nabla h(x_0) = 0$).} By defining the prox-center of the set, notice that $h(z_{k+1}) \geq \frac{1}{2}\|x_0 - z_{k+1}\|^2$. Therefore, from \eqref{InEq:DualAvgBound}, we obtain the bound
\begin{align}\label{Eq:DualAvgBound3}
\gamma_{k+1} D_h(x^\ast, z_{k+1}) - \gamma_k D_h(x^\ast, z_k)& \leq \gamma_k D_h(z_{k+1}, z_k)  +  \langle \gamma_{k+1} \nabla h(z_{k+1}) - \gamma_k \nabla h(z_k), z_{k+1} - x^\ast  \rangle\notag \\&\quad + (\gamma_{k+1} - \gamma_{k})h(x^\ast).
\end{align}
%
%
With this inequality in hand, the proof of convergence is essentially equivalent to the proof of Theorem~\ref{Thm:Quasi}. In particular,  denoting $D^\ast_{\gamma_{k+1}, \gamma_k} = \gamma_{k+1} D_h(x^\ast, z_{k+1}) - \gamma_{k} D_h(x^\ast, z_k)$, we obtain the following upper bound, 
%\begin{small}
\begin{subequations}
\begin{align}
D^\ast_{\gamma_{k+1}, \gamma_k}& \overset{\eqref{Eq:DualAvgBound3}}{\leq} \gamma_{k} D_h(z_{k+1}, z_k) + \langle \gamma_{k+1} \nabla h(z_{k+1}) - \gamma_k \nabla h(z_k), z_{k+1} - x^\ast\rangle +  (\gamma_{k+1} - \gamma_{k})h(x^\ast) \notag\\ 
&\overset{\eqref{Eq:DualAvgX}}{\leq} -\frac{\gamma_{k}}{2}\|z_k - z_{k+1}\|^2 - (A_{k+1} - A_k) \langle \nabla f(x_{k+1}), z_{k+1} - x^\ast\rangle + (\gamma_{k+1} - \gamma_{k})h(x^\ast)  \notag\\
&= -\frac{\gamma_{k}}{2}\|z_k - z_{k+1}\|^2 - (A_{k+1} - A_k) \langle \nabla f(x_{k+1}), z_{k+1} - z_k\rangle - (A_{k+1} - A_k)\langle \nabla f(x_{k+1}), z_k - x^\ast\rangle\notag\\
&\quad   +(\gamma_{k+1} - \gamma_{k})h(x^\ast) \notag \\
&\leq\frac{(A_{k+1} - A_k)^2}{2\gamma_{k}} \|\nabla f(x_{k+1})\|_\ast^2 - (A_{k+1} - A_k) \langle \nabla f(x_{k+1}), z_k - x^\ast\rangle + (\gamma_{k+1} - \gamma_{k})h(x^\ast)  \notag\\
& \overset{\eqref{Eq:ZSeqQuasi}}{=}\frac{(A_{k+1} - A_k)^2}{2\gamma_{k}} \|\nabla f(x_{k+1})\|_\ast^2- (A_{k+1} - A_k)\langle \nabla f(x_{k+1}), \frac{A_{k+1}}{(A_{k+1} - A_k)}(x_{k+1} - x_k) + x_k - x^\ast\rangle\notag\\
&\quad +(\gamma_{k+1} - \gamma_{k})h(x^\ast)  \notag \\
& =\frac{(A_{k+1} - A_k)^2}{2\gamma_{k}} \|\nabla f(x_{k+1})\|_\ast^2 - (A_{k+1} - A_k)\langle \nabla f(x_{k+1}), x_{k+1} - x^\ast\rangle \notag \\
&\quad + (A_{k+1} - A_k)\langle \nabla f(x_{k+1}), x_{k+1} - x_k \rangle- A_{k+1} \langle \nabla f(x_{k+1}), x_{k+1} - x_k\rangle  +(\gamma_{k+1} - \gamma_{k})h(x^\ast)  \notag\\
& = \frac{(A_{k+1} - A_k)^2}{2\gamma_{k}} \|\nabla f(x_{k+1})\|_\ast^2  - (A_{k+1} - A_k)\langle \nabla f(x_{k+1}), x_{k+1} - x^\ast\rangle - A_{k} \langle \nabla f(x_{k+1}), x_{k+1} - x_k\rangle\notag \\
&\quad + (\gamma_{k+1} - \gamma_{k})h(x^\ast)  \notag \\
& = -A_k(f(x_{k+1}) - f(x_k)) - (A_{k+1} - A_k)(f(x_{k+1}) - f(x^\ast)) + \frac{(A_{k+1} - A_k)^2}{2\gamma_{k}} \|\nabla f(x_{k+1})\|_\ast^2\notag\\
&\quad + (\gamma_{k+1} - \gamma_{k})h(x^\ast)\notag\\
& = A_k (f(x_k) - f(x^\ast)) - A_{k+1}(f(x_{k+1}) - f(x^\ast)) + \frac{(A_{k+1} - A_k)^2}{2\gamma_{k}} \|\nabla f(x_{k+1})\|_\ast^2\notag\\
&\quad+ (\gamma_{k+1} - \gamma_{k})D_h(x^\ast, x_0),\notag
\end{align}
\end{subequations}
%\end{small}
where in the last inequality, we use Fenchel-Young. Therefore, for~\eqref{Eq:Lyap3} we can ensure the following
\begin{align*}
E_{k+1} - E_k & \leq \frac{(A_{k+1} - A_k)^2}{2\gamma_{k}} \|\nabla f(x_{k+1})\|_\ast^2\leq   \frac{(A_{k+1} - A_k)^2}{2\gamma_{k}} G^2,
\end{align*}
where $h(x^\ast) = D_h(x^\ast, x_0)$ follows from the definition of the prox-center. Taking $\gamma_{-1} = \gamma_0 = 1$ and choosing $\sum_{i=1}^{k} \frac{(A_{i} - A_{i-1})^2}{\gamma_{i-1}}  < \infty$, we obtain the convergence rate 
% \begin{align*}
%E_{k} \leq E_0 + \frac{1}{2}\sum_{i=0}^{k}\frac{(A_{i+1} - A_i)^2}{\gamma_{i-1}} G^2 + (\gamma_k - \gamma_{-1})D_h(x^\ast, x_0),
%\end{align*}
%which we can write as,
\begin{align*}
f(x_k) - f(x^\ast) \leq \frac{A_0(f(x_0) - f(x^\ast)) +  \gamma_k D_h(x^\ast , x_0) +\frac{1}{2}\sum_{i=1}^{k}\frac{(A_{i} - A_{i-1})^2}{\gamma_{i-1}}G^2}{A_k}.
\end{align*}
This matches the bound Nesterov obtained in \cite{Nesterov15}. Note that a very similar analysis can be presented for Nesterov's dual averaging algorithm~\cite{Nesterov09}.

%%%%%%%%%%%%%%%%%%%%FRANK WOLFE PROOF %%%%%%%%%%%%%
\subsection{Proof of Discretizations}
\label{App:OtherDiscret}
\subsubsection{Proof of Method 1} We analyze the algorithm~\eqref{Eq:QuasiSub2} using the Lyapunov function~\eqref{Eq:DiscLyapFunc1}: 
\begin{subequations}
\begin{align}
E_{k+1} - E_k %&= A_{k+1}(f(x_{k+1}) -f(x^\ast)) - A_k(f(x_k) - f(x^\ast)) + D_h(x^\ast, z_{k+1}) - D_h(x^\ast, z_k) \notag\\
&= A_{k+1}(f(x_{k+1}) -f(x_k)) + \alpha_k(f(x_k) - f(x^\ast)) - \langle \nabla h(z_{k+1}) - \nabla h(z_k), x^\ast - z_{k+1} \rangle\notag\\
&\quad- D_h(z_{k+1}, z_k) \notag\\
 &= A_{k+1}(f(x_{k+1}) -f(x_k)) + \alpha_k(f(x_k) - f(x^\ast)) + (A_{k+1} - A_k)\langle \nabla f(x_k), x^\ast - x_{k+1}\rangle \notag\\
 &\quad -A_{k} \langle \nabla f(x_k), x_{k+1} - x_k\rangle - D_h(z_{k+1}, z_k) \label{Eq:Stuff}\\
 &= A_{k+1}(f(x_{k+1}) -f(x_k)) + \alpha_k(f(x_k) - f(x^\ast)) + (A_{k+1} - A_k)\langle \nabla f(x_k), x^\ast - x_{k}\rangle \notag\\
 &\quad  + (A_{k+1} - A_k)\langle \nabla f(x_k), x_{k}- x_{k+1}\rangle -A_{k} \langle \nabla f(x_k), x_{k+1} - x_k\rangle - D_h(z_{k+1}, z_k)\notag\\
  &= A_{k+1}(f(x_{k+1}) -f(x_k) - \langle \nabla f(x_k), x_{k+1} - x_k\rangle) + \alpha_k(f(x_k) - f(x^\ast)+\langle \nabla f(x_k), x^\ast - x_{k}\rangle)  \notag \\ &\quad- D_h(z_{k+1}, z_k) \notag\\
 & \leq A_{k+1} \frac{L}{2}\| x_{k+1} - x_{k}\|^2 \notag\\
% & =\frac{1}{2} \frac{A_{k+1}\alpha_k^2}{A_{k}^2} \|x_{k+1} - z_{k+1}\|^ 2\\
& \overset{\eqref{Eq:XSeqOther1}}{\leq}  \frac{L}{2} \frac{A_{k+1}\alpha_k^2}{A_{k}^2}diam(\X)^2,
\end{align}
\end{subequations}
%\end{small}
where in~\eqref{Eq:Stuff} we used~\eqref{Eq:XSeqOther1} and~\eqref{Eq:ZSeqOther1}. Therefore, we obtain the following upper bound, 
\begin{align*}
f(x_k) - f(x^\ast) \leq \frac{A_0(f(x_0)- f(x^\ast)) + D_h(x^\ast, x_0)  +  \frac{L}{2}\sum_{i=1}^k \frac{A_{i}(A_{i} - A_{i-1})^2}{A_{i-1}^2}diam(\X)^2 }{A_k}.
\end{align*}
Choosing $A_k = \frac{k(k+1)}{2}$ gives a $O(1/k)$ convergence rate. 

\subsubsection{Proof of Method 2}  We analyze ~\eqref{Eq:QuasiSub3} using the Lyapunov function~\eqref{Eq:DiscLyapFunc1}. Denoting $D^\ast_{k+1, k} = D_h(x^\ast,z_{k+1})- D_h(x^\ast,z_{k})$, observe that
%\begin{small}
\begin{subequations}%\label{Eq:LyapDiscArg}
\begin{align}
D^\ast_{k+1, k} &= -\langle \nabla h(z_{k+1}) - \nabla h(z_k), x^\ast - z_{k+1}\rangle - D_h(z_{k+1}, z_k)\notag\\
%&{\color{red} \overset{\eqref{Eq:ELBreg}}{=}e^{\alpha_t + \beta_t} \langle \nabla f(X_t), x - X_t \rangle\, - e^{ \beta_t} \langle \nabla f(X_t), \dot X\rangle}\, \notag   \\
& \overset{\eqref{Eq:XSeqMethod2}}{=} (A_{k+1} - A_k) \langle \nabla f(x_{k+1}), x^\ast - z_{k}\rangle  + (A_{k+1} - A_k) \langle \nabla f(x_{k+1}), z_{k} - z_{k+1}\rangle - D_h(z_{k+1}, z_k)\notag\\
& \overset{\eqref{Eq:ZSeqMethod2}}{=}(A_{k+1} - A_k) \langle \nabla f(x_{k+1}), x^\ast - x_{k+1}\rangle + A_k \langle \nabla f(x_{k+1}), x_{k+1} - x_k\rangle\notag \\&\quad- D_h(z_{k+1}, z_k)\notag\\
& \leq(A_{k+1} - A_k) \langle \nabla f(x_{k+1}), x^\ast - x_{k+1}\rangle + A_k \langle \nabla f(x_{k+1}), x_{k+1} - x_k\rangle+ \frac{(A_{k+1} - A_k)^2}{2} \|\nabla f(x_{k+1})\|^2 \notag\\
%
&\leq (A_{k+1} - A_k)(f(x^\ast) - f(x_{k+1})) + A_k (f(x_k) - f(x_{k+1}))  + \frac{(A_{k+1} - A_k)^2}{2} \|\nabla f(x_{k+1})\|^2 \notag\\
%& {\color{red}= - \frac{d}{dt}\left\{ e^{\beta_t}\left(f(X_t) - f(x)\right)\right\}\label{Eq:cond1}}\\
& = A_k (f(x_k) - f(x^\ast)) - A_{k+1}(f(x_{k+1}) - f(x^\ast))  + \frac{(A_{k+1} - A_k)^2}{2} \|\nabla f(x_{k+1})\|^2 \notag.
\end{align}
\end{subequations}
Therefore, 
\begin{align*}
E_{k+1} - E_k \leq  \frac{(A_{k+1} - A_k)^2}{2} \|\nabla f(x_{k+1})\|^2 \leq  \frac{(A_{k+1} - A_k)^2}{2} G^2
\end{align*}
and we have the following convergence rate guarantee
\begin{align*}
f(x_k) - f(x^\ast) \leq \frac{ A_0(f(x_0)- f(x^\ast))+D_h(x^\ast, x_0)+ \sum_{i=1}^k\frac{(A_{i} - A_{i-1})^2}{2} G^2}{A_k},
\end{align*}
which is the same convergence rate as the quasi-monotone subgradient method~\eqref{Eq:QuasiSub}. 

%%%%%%%%%%%%%%%%%%%%FRANK WOLFE PROOF %%%%%%%%%%%%%
\subsection{Proof of Conditional Gradient Method}
\label{App:CondMethod}
We use the Lyapunov function \eqref{Eq:FrankWolfeLyap} to analyze conditional gradient method~\eqref{eq:Frank-WolfeAlgo}:
\label{App:ProofAccGrad}
\begin{subequations}
\begin{align*}
E_{k+1} - E_k &= A_{k+1}(f(x_{k+1}) -f(x^\ast)) - A_k(f(x_k) - f(x^\ast)) \notag\\
& = A_{k+1}(f(x_{k+1}) -f(x_k)) - \alpha_{k+1}(f(x_k) - f(x^\ast))\notag\\
&  \leq A_{k+1}(\langle \nabla f(x_k), x_{k+1} - x_k \rangle + \frac{L}{2}\|x_{k+1} - x_k\|^2) - \alpha_{k+1} (f(x_k) - f(x^\ast)) \\
& = A_{k+1}(\tau_k\langle \nabla f(x_k), z_k- x_k \rangle + \frac{L\tau_k^2}{2}\|z_k - x_k\|^2) - \alpha_{k+1} (f(x_k) - f(x^\ast)) \\
&= \alpha_{k+1} \langle \nabla f(x_k), z_k- x_k \rangle + \frac{L}{2}\frac{\alpha_{k+1}^2}{A_{k+1}}\|z_k - x_k\|^2 - \alpha_{k+1} (f(x_k) - f(x^\ast))\\
& \leq \alpha_{k+1} ( f(x^\ast) - f(x_k) + \langle \nabla f(x_k), x^\ast -  x_k \rangle) + \frac{L}{2}\frac{\alpha_{k+1}^2}{A_{k+1}}\|z_k - x_k\|^2  \\
& \leq \frac{L}{2}\frac{\alpha_{k+1}^2}{A_{k+1}}diam(\X)^2.
\end{align*}
\end{subequations}
Therefore, 
\begin{align*}
E_k \leq E_0 + \frac{L}{2} diam(\X)^2\sum_{i=0}^k \frac{\alpha_{i}^2}{A_{i}}.
\end{align*}
Choosing $A_{k+1} = \frac{k(k+1)}{2}$ gives the coupling term $\tau_k = \frac{2}{k+2}$ and $O(1/k)$ convergence rate. This recovers the bounds shown by Nesterov~\cite{NesterovCond15}. 


\paragraph{Proof of extension}
Notice that our variational condition~\eqref{Eq:Var2Cond} (taking $y = (1- \tau_k)x_k + \tau_k x^\ast$, where as usual, $\tau_k = \frac{\alpha_k}{A_{k+1}}$, and $\alpha_k = A_{k+1} - A_k$) gives the inequality 
\begin{align}\label{Eq:In1}
-\alpha_k\langle \nabla f(x_k), x^\ast - x_k\rangle &\leq - A_{k+1}\langle \nabla^2f(x_k)(x_{k+1} - x_k), x_{k+1} - x_k\rangle + \alpha_k \langle \nabla^2f(x_k)(x_{k+1} - x_k), x^\ast - x_k\rangle\notag\\
&\quad  + A_k \langle \nabla f(x_k), x_k - x_{k+1}\rangle.
\end{align}
Noting this, we have 
\begin{subequations}
\begin{align*}
E_{k+1} - E_k 
& = A_{k+1}(f(x_{k+1}) -f(x_k)) - \alpha_k(f(x_k) - f(x^\ast))\notag\\
&\leq A_{k+1}(f(x_{k+1}) -f(x_k)) - \alpha_k\langle \nabla f(x_k), x^\ast - x_k\rangle\\
&\overset{\eqref{Eq:In1}}{\leq} A_{k+1}(f(x_{k+1}) -f(x_k)) - A_{k+1}\langle \nabla f(x_k), x_{k+1}- x_k\rangle\\
&\quad  - A_{k+1} \langle \nabla^2 f(x_k)(x_{k+1} - x_k), x_{k+1} - x_k \rangle  + \alpha_k \langle \nabla^2f(x_k)(x_{k+1} - x_k), x^\ast - x_k\rangle\\
&\leq A_{k+1}(f(x_{k+1}) -f(x_k)) - A_{k+1}\langle \nabla f(x_k), x_{k+1}- x_k\rangle \\
&\quad  - A_{k+1} \langle \nabla^2 f(x_k)(x_{k+1} - x_k), x_{k+1} - x_k \rangle+ \frac{\alpha_k^2}{A_k} \langle \nabla^2f(x_k)(z_k - x_k), z_k - x_k\rangle\\
&\leq A_{k+1}(f(x_{k+1}) -f(x_k) - \langle \nabla f(x_k), x_{k+1}- x_k\rangle - \frac{1}{2}\langle \nabla^2 f(x_k)(x_{k+1} - x_k), x_{k+1} - x_k \rangle)\\
&\quad+\frac{\alpha_k^2}{A_k} LD^2\\
&\leq A_{k+1}\frac{L}{6} \|x_{k+1} - x_k\|^3 +\frac{\alpha_k^2}{A_k} LD^2\\
&\leq\frac{\alpha_k^3}{A_k^2}\frac{L}{6} D^3 +\frac{\alpha_k^2}{A_k} LD^2,
\end{align*}
\end{subequations}
recovering the bound shown by Nesterov~\cite[(5.4)]{NesterovCond15}. We remark that the following proof can easily be extended to incorporate the setting where $f$ has  Holder-continuous Hessians as in~\cite{NesterovCond15}.





%%%%%%%%%%%%%%PROOF OF ACCELERATED
\subsection{Proof of Accelerated Gradient Methods}
\label{App:ProofAccGrad}
%\subsection{Accelerated Gradient Descent}
%\label{Sec:AccGrad}
We present the generalized accelerated gradient descent algorithm~\cite{Baes09,Acceleration} first outlined by Michel Baes (for the weakly convex setting).  The algorithm maintains the following sequences:
\begin{algorithm}[H]
\begin{subequations}\label{Eq:AcceleratedGrad}
\caption{Accelerated Gradient Descent (Weakly Convex Setting)}
{\bf Assumptions:} $f, h$ are convex and differentiable. $h$ satisfies smoothness condition $ \frac{1}{p}\|x - y\|^p \leq D_h(x,y)$ and $f$ satisfies smoothness condition $\|\nabla^{p} f\|\leq L$.\\
Choose $A_0 = 1$, $M>0$, $\tilde A_{k+1} = L^{-1} A_{k+1}$,  $\tau_k = \frac{\tilde A_{k+1}- \tilde A_k}{\tilde A_{k+1}}:= \frac{\alpha_k}{\tilde A_{k+1}}$ and $x_0 = z_0 = y_0$. Define recursively, 
\begin{align}
x_{k+1} &= \tau_k z_k + (1- \tau_k)  y_k\label{Eq:ZSeqAcc1}\\
z_k &= \arg \min_{z\in \X} \left\{ \sum_{i=1}^k \alpha_i \langle \nabla f(y_i), z\rangle + D_h(z, z_0)\right\}\label{Eq:XSeqAcc1}
\end{align}
$y_{k}$ is a gradient update 
\begin{align}
G_{p,\epsilon,N}(x_k) = \arg \min_y \left\{ f_{p-1}(y;x_k) + \frac{LN}{p}\|y-x_k\|^p\right\},
\end{align}
\end{subequations}
where 
\begin{equation*}
f_{p-1}(y;x) = \sum_{i=0}^{p-1} \frac{1}{i!} \nabla^i f(x)(y-x)^i = f(x) + \langle \nabla f(x), y-x\rangle + \cdots + \frac{1}{(p-1)!} \nabla^{p-1} f(x)(y-x)^{p-1}
\end{equation*}

\end{algorithm}
\noindent This algorithm satisfies the following optimality conditions, 
\begin{subequations}
\begin{align}
z_{k} &= y_k + \frac{\tilde A_{k+1}}{\tilde A_{k+1} -\tilde A_{k}} (x_{k+1} - y_k),\label{Eq:ZSeqAcc}\\
\nabla h(z_{k+1}) - \nabla h(z_{k}) &= -(\tilde A_{k+1} -  \tilde A_{k})  \nabla f(y_{k+1})\label{Eq:XSeqAcc}\\
\quad M \|L^{-1}\nabla f(y_{k})\|_\ast^{\frac{p}{p-1}} &\leq L^{-1}\langle\nabla f(y_{k}), x_{k} - y_{k}\rangle \label{Eq:YSeqAcc}
\end{align}
\end{subequations}
where $M = \frac{(N^2 - 1)^{\frac{p-2}{2p-2}}}{2N}$. Denoting $D^\ast_{k+1, k} = D_h(x^\ast,z_{k+1})- D_h(x^\ast,z_{k})$, we obtain the following bound:
%\label{App:ProofQuasi}
%\begin{}
\begin{subequations}
\begin{align}
D^\ast_{k+1, k} &= -D_h(z_{k+1}, z_k) +  \langle \nabla h(z_{k+1}) - \nabla h(z_k), z_{k+1} - x^\ast\rangle\notag \\
&\overset{\eqref{Eq:XSeqAcc}}{\leq} -\frac{1}{p}\|z_k - z_{k+1}\|^p -(\tilde A_{k+1} -\tilde A_{k}) \langle  \nabla f(y_{k+1}), z_{k+1} - x^\ast\rangle \label{Eq:StrongConvAssump} \\
&= -\frac{1}{p}\|z_k - z_{k+1}\|^p - (\tilde A_{k+1} -\tilde A_{k}) \langle \nabla f(y_{k+1}), z_{k+1} - z_k\rangle -(\tilde A_{k+1} -\tilde A_{k}) \langle  \nabla   f(y_{k+1}), z_k - x^\ast\rangle \notag\\
&\leq\frac{p-1}{p}(\tilde A_{k+1} -\tilde A_{k})^{\frac{p}{p-1}} \|\nabla  f(y_{k+1})\|_\ast^{\frac{p}{p-1}} - (\tilde A_{k+1} -\tilde A_{k}) \langle\nabla  f(y_{k+1}), z_k - x^\ast\rangle \label{Eq:HoldBound}\\
& \overset{\eqref{Eq:ZSeqAcc}}{=}\frac{p-1}{p}(\tilde A_{k+1} -\tilde A_{k})^{\frac{p}{p-1}} \|\nabla f(y_{k+1})\|_\ast^{\frac{p}{p-1}}
 - (\tilde A_{k+1} - \tilde A_k) \langle\nabla  f(y_{k+1}), \frac{\tilde A_{k+1}}{(\tilde A_{k+1} -\tilde A_{k})} x_{k+1} - y_k\rangle\notag \\
& \quad -(\tilde A_{k+1} - \tilde A_k) \langle\nabla  f(y_{k+1}),  y_k - x^\ast\rangle \notag\\
 %
&=\frac{p-1}{p}(\tilde A_{k+1} -\tilde A_{k})^{\frac{p}{p-1}} \|\nabla f(y_{k+1})\|_\ast^{\frac{p}{p-1}}- \tilde A_{k+1} \langle\nabla  f(y_{k+1}), x_{k+1} - y_k\rangle  \notag\\
&\quad- (\tilde A_{k+1} -\tilde A_{k})\langle\nabla  f(y_{k+1}), y_{k+1} - x^\ast\rangle + (\tilde A_{k+1} -\tilde A_{k})\langle \nabla f(y_{k+1}), y_{k+1} - y_k \rangle \notag\\
%
& =\frac{p-1}{p}(\tilde A_{k+1} -\tilde A_{k})^{\frac{p}{p-1}} \|\nabla f(y_{k+1})\|_\ast^{\frac{p}{p-1}}- \tilde A_{k+1} \langle\nabla f(y_{k+1}), x_{k+1} - y_{k+1}\rangle\notag \\
 &- (\tilde A_{k+1} -\tilde A_{k})\langle\nabla  f(y_{k+1}), y_{k+1} - x^\ast\rangle + (\tilde A_{k+1} -\tilde A_{k})\langle \nabla   f(y_{k+1}), y_{k+1} - y_k \rangle \\
 &\quad- \tilde A_{k+1} \langle\nabla   f(y_{k+1}), y_{k+1} - y_{k}\rangle \notag\\
& =\frac{p-1}{p}(\tilde A_{k+1} -\tilde A_{k})^{\frac{p}{p-1}} \|\nabla  f(y_{k+1})\|_\ast^{\frac{p}{p-1}}-\tilde  A_{k+1} \langle\nabla f(y_{k+1}), x_{k+1} - y_{k+1}\rangle \notag\\
 &\quad + (A_{k+1} -A_{k})\langle\nabla \tilde  f(y_{k+1}),  x^\ast - y_{k+1}\rangle
 -\tilde A_{k}\langle \nabla f(y_{k+1}), y_{k+1} - y_k \rangle \notag\\
& \overset{\eqref{Eq:YSeqAcc}}{\leq}\frac{p-1}{p}(\tilde A_{k+1} -\tilde A_{k})^{\frac{p}{p-1}} \|\nabla f(y_{k+1})\|_\ast^{\frac{p}{p-1}} - \tilde A_{k+1}L^{-\frac{1}{p-1}}M \|\nabla f(y_{k+1})\|_\ast^{\frac{p}{p-1}}   \notag\\
&\quad  + (\tilde A_{k+1} -\tilde A_{k})\langle\nabla   f(y_{k+1}),  x^\ast - y_{k+1}\rangle-\tilde A_{k}\langle \nabla  f(y_{k+1}), y_{k+1} - y_k \rangle \notag\\
& \leq -\tilde A_k( f(y_{k+1}) -f(y_k)) - (\tilde A_{k+1} -\tilde A_{k})(f(y_{k+1}) - f(x^\ast)) \notag\\
&\quad+  \frac{p-1}{p}(\tilde A_{k+1} -\tilde A_{k})^{\frac{p}{p-1}} \|\nabla f(y_{k+1})\|_\ast^{\frac{p}{p-1}}  - \tilde A_{k+1}L^{-\frac{1}{p-1}} M \|\nabla f(y_{k+1})\|_\ast^{\frac{p}{p-1}}  \notag \\
& = \tilde  A_k (f(y_k) - f(x)) -  \tilde A_{k+1}( f(y_{k+1}) -  f(x^\ast)) \notag \\
&\quad+\left(\frac{p-1}{p}(A_{k+1} -A_{k})^{\frac{p}{p-1}} - A_{k+1}M\right) L^{-\frac{p}{p-1}}\|\nabla f(y_{k+1})\|_\ast^{\frac{p}{p-1}} \notag,
\end{align}
\end{subequations}
where in \eqref{Eq:StrongConvAssump} we have used the uniform convexity assumption and in \eqref{Eq:HoldBound} we have used the Fenchel-Young inequality. Therefore, for the general accelerated gradient methods we have the following bound 
\begin{align*}
E_{k+1} - E_k \leq \varepsilon_{k+1},
\end{align*}
where the error scales in the following way
\begin{equation*}
\varepsilon_{k+1} =\left(\frac{p-1}{p}(A_{k+1} -A_{k})^{\frac{p}{p-1}} - A_{k+1}M\right) L^{-\frac{p}{p-1}}\|\nabla  f(y_{k+1})\|_\ast^{\frac{p}{p-1}}.
\end{equation*}
Choosing the error to be nonpositive means that $A_k$ can be at most a polynomial of degree $p$. 

\subsection{Accelerated Gradient Descent: Version 2}
We now analyze the second version of the accelerated gradient descent algorithm using the same energy functional~\eqref{Eq:LyapAccGrad}:
\begin{align*}
E_{k+1} - E_k &= \tilde A_{k+1}(f(y_{k+1}) - f(x^\ast)) - \tilde A_{k}(f(y_{k}) - f(x^\ast)) + D_h(x^\ast, z_{k+1}) - D_h(x^\ast, z_{k}) \\
& = \tilde A_{k+1}(f(y_{k+1}) - f(x_{k+1})) + \tilde A_{k}(f(x_{k+1}) - f(y_{k})) + (\tilde A_{k+1} - \tilde A_k)(f(x_{k+1}) - f(x^\ast)) \\
&\quad - \langle \nabla h(z_{k+1}) - \nabla h(z_k), x^\ast - z_{k+1}\rangle - D_h(z_{k+1}, z_k)\\
& \overset{\eqref{Eq:XSeqAcc4}}{\leq} \tilde A_{k+1}(f(y_{k+1}) - f(x_{k+1})) + \tilde A_{k}(f(x_{k+1}) - f(y_{k})) + (\tilde A_{k+1} - \tilde A_k)(f(x_{k+1}) - f(x^\ast)) \\
&\quad + (\tilde A_{k+1} - \tilde A_k)\langle \nabla f(x_{k+1}), x^\ast - z_{k+1}\rangle - \frac{1}{2}\|z_{k+1} - z_k\|^2\\
& = \tilde A_{k+1}(f(y_{k+1}) - f(x_{k+1})) + \tilde A_{k}(f(x_{k+1}) - f(y_{k})) + (\tilde A_{k+1} - \tilde A_k)(f(x_{k+1}) - f(x^\ast)) \\
&\quad  + (\tilde A_{k+1} - \tilde A_k)\langle \nabla f(x_{k+1}), x^\ast - z_{k}\rangle + (\tilde A_{k+1} - \tilde A_k)\langle \nabla f(x_{k+1}),  z_{k} - z_{k+1}\rangle- \frac{1}{2}\|z_{k+1} - z_k\|^2\\
& \leq \tilde A_{k+1}(f(y_{k+1}) - f(x_{k+1})) + \tilde A_{k}(f(x_{k+1}) - f(y_{k})) + (\tilde A_{k+1} - \tilde A_k)(f(x_{k+1}) - f(x^\ast)) \\
&\quad  + (\tilde A_{k+1} - \tilde A_k)\langle \nabla f(x_{k+1}), x^\ast - z_{k}\rangle + \frac{(\tilde A_{k+1} - \tilde A_k)^2}{2}\|\nabla f(x_{k+1})\|^2 \\
& \overset{\eqref{Eq:ZSeqAcc4}}{=}\tilde A_{k+1}(f(y_{k+1}) - f(x_{k+1})) + \tilde A_{k}(f(x_{k+1}) - f(y_{k})) + (\tilde A_{k+1} - \tilde A_k)(f(x_{k+1}) - f(x^\ast)) \\
&\quad  + \tilde A_{k+1} \langle \nabla f(x_{k+1}),  y_k - x_{k+1}\rangle + (\tilde A_{k+1} - \tilde A_k)\langle \nabla f(x_{k+1}), x^\ast - y_k\rangle + \frac{(\tilde A_{k+1} - \tilde A_k)^2}{2}\|\nabla f(x_{k+1})\|^2 \\
& =\tilde A_{k+1}(f(y_{k+1}) - f(x_{k+1})) + \tilde A_{k}(f(x_{k+1}) - f(y_{k})) + (\tilde A_{k+1} - \tilde A_k)(f(x_{k+1}) - f(x^\ast)) \\
&\quad  + \tilde A_{k} \langle \nabla f(x_{k+1}),  y_k - x_{k+1}\rangle + (\tilde A_{k+1} - \tilde A_k)\langle \nabla f(x_{k+1}), x^\ast - x_{k+1}\rangle + \frac{(\tilde A_{k+1} - \tilde A_k)^2}{2}\|\nabla f(x_{k+1})\|^2 \\
& \overset{\eqref{Eq:YSeqAcc4}}{\leq} - \frac{\tilde A_{k+1}}{2L}\|\nabla f(x_{k+1})\|^2 + \tilde A_{k}(f(x_{k+1}) - f(y_{k})) + (\tilde A_{k+1} - \tilde A_k)(f(x_{k+1}) - f(x^\ast)) \\
&\quad  + \tilde A_{k} \langle \nabla f(x_{k+1}),  y_k - x_{k+1}\rangle + (\tilde A_{k+1} - \tilde A_k)\langle \nabla f(x_{k+1}), x^\ast - x_{k+1}\rangle + \frac{(\tilde A_{k+1} - \tilde A_k)^2}{2}\|\nabla f(x_{k+1})\|^2 \\
& \leq \frac{1}{2L^2}\left((A_{k+1} -  A_k)^2 - A_{k+1} \right) \|\nabla f(x_{k+1})\|^2.
\end{align*}
Therefore we have shown 
\begin{align*}
E_{k+1} - E_k \leq \varepsilon_{k+1}, 
\end{align*}
where 
\begin{align*}
\varepsilon_{k+1} = \frac{1}{2L^2}\left[(A_{k+1} -  A_k)^2 - A_{k+1} \right] \|\nabla f(x_{k+1})\|^2.
\end{align*}
To ensure our error $\varepsilon_k$ is nonpositive, $A_k$ can be at most a polynomial of degree two. %Note, this matches the bound for our theorem with $M = \frac{1}{2}$ (or, $N = 1$) 

\section{Strong Convexity}
\label{App:StrongConvexity}


\subsection{Proof of Accelerated Gradient \eqref{Eq:StrongConv1}}
%\begin{proof}
\begin{align*}
\tilde E_{k+1} - \tilde E_k &= f(y_{k+1}) - f(y_k) - \mu \langle z_{k+1} - z_k, x^\ast - z_{k+1}\rangle - \frac{\mu}{2}\|z_{k+1} - z_k\|^2\\
&= f(y_{k+1}) - f(x_{k+1}) + f(x_{k+1}) - f(y_k) - \mu \langle z_{k+1} - z_k, x^\ast - z_{k}\rangle + \frac{\mu}{2}\|z_{k+1} - z_k\|^2\\
&= f(y_{k+1})- f(x_{k+1}) + \langle \nabla f(x_{k+1}), x_{k+1} - y_k\rangle - \frac{\mu}{2}\|x_{k+1} - y_k\|^2 \\
&\quad + \tau_k \langle \nabla f(x_{k+1}) - \mu(x_{k+1} - z_k), x^\ast - z_{k}\rangle + \frac{\mu}{2}\|z_{k+1} - z_k\|^2\\
&\overset{\eqref{Eq:ZSeq2}}{=} f(y_{k+1}) - f(x_{k+1}) + \langle \nabla f(x_{k+1}), x_{k+1} - y_k\rangle - \frac{\mu}{2}\|x_{k+1} - y_k\|^2  + \tau_k \langle \nabla f(x_{k+1}), x^\ast - x_{k+1} \rangle \\
&\quad - \tau_k\mu\langle x_{k+1} - z_k, x^\ast - z_{k}\rangle +\tau_k \langle\nabla f(x_{k+1}), x_{k+1} - z_{k}\rangle  + \frac{\mu}{2}\|z_{k+1} - z_k\|^2\\
%
&\leq -\tau_k\left( f(x_{k+1} ) - f(x^\ast) + \frac{\mu}{2}\|x^\ast - x_{k+1}\|^2 \right) + f(y_{k+1}) - f(x_{k+1})   - \tau_k\mu\langle x_{k+1} - z_k, x^\ast - z_{k}\rangle\\
&\quad +\tau_k \langle\nabla f(x_{k+1}), x_{k+1} - z_{k}\rangle  + \frac{\mu}{2}\|z_{k+1} - z_k\|^2 + \langle \nabla f(x_{k+1}), x_{k+1} - y_k\rangle - \frac{\mu}{2}\|x_{k+1} - y_k\|^2\\
&= -\tau_k\left( f(x_{k+1} ) - f(x^\ast) + \frac{\mu}{2}\|x^\ast - z_k\|^2 \right) + f(y_{k+1}) - f(x_{k+1})  +\tau_k \langle\nabla f(x_{k+1}), x_{k+1} - z_{k}\rangle\\
&\quad   + \frac{\mu}{2}\|z_{k+1} - z_k\|^2 - \frac{\mu\tau_k}{2}\|z_k - x_{k+1}\|^2 + \langle \nabla f(x_{k+1}), x_{k+1} - y_k\rangle - \frac{\mu}{2} \|x_{k+1} - y_{k}\|^2 \\
&\overset{\eqref{Eq:Coupling2}}{=} -\tau_k\left( f(x_{k+1} ) - f(x^\ast) + \frac{\mu}{2}\|x^\ast - z_k\|^2 \right) + f(y_{k+1}) - f(x_{k+1}) - \frac{\mu}{2\tau_k}\|y_k - x_{k+1}\|^2  \\
&\quad   + \frac{\mu}{2}\|z_{k+1} - z_k\|^2  - \frac{\mu}{2} \|x_{k+1} - y_{k}\|^2\\\
&\overset{\eqref{Eq:ZSeq2}}{=} -\tau_k\left( f(x_{k+1} ) - f(x^\ast) + \frac{\mu}{2}\|x^\ast - z_k\|^2 \right) + f(y_{k+1}) - f(x_{k+1})   - \frac{\mu}{2\tau_k}\|y_k - x_{k+1}\|^2  \\
&\quad   + \frac{\mu}{2}\|\tau_k(x_{k+1} - z_k)\|^2 + \tau_k \langle \nabla f(x_{k+1}), \tau_k(x_{k+1} - z_k)\rangle + \frac{\tau_k^2}{2\mu}\|\nabla f(x_{k+1})\|^2  - \frac{\mu}{2} \|x_{k+1} - y_{k}\|^2 \\
&\leq -\tau_k\left( f(x_{k+1} ) - f(x^\ast) + \frac{\mu}{2}\|x^\ast - z_k\|^2 \right) + f(y_{k+1}) - f(x_{k+1})   - \frac{\mu}{2\tau_k}\|y_k - x_{k+1}\|^2      \\
&\quad  + \tau_k \langle \nabla f(x_{k+1}), y_k - x_{k+1}\rangle + \frac{\tau_k^2}{2\mu}\|\nabla f(x_{k+1})\|^2  \\
&\leq -\tau_k\left( f(x_{k+1} ) - f(x^\ast) + \frac{\mu}{2}\|x^\ast - z_k\|^2 \right)  -\tau_k( f(y_k) - f(x_{k+1})) + \left( \frac{L\tau_k}{2}- \frac{\mu}{2\tau_k}\right)\|x_{k+1} - y_k\|^2  \\
&\quad + \left(\frac{\tau_k^2}{2\mu}-\frac{1}{2L}\right)\|\nabla f(x_{k+1})\|^2   \\
&=-\tau_k\left( f(y_k) - f(x^\ast) + \frac{\mu}{2}\|x^\ast - z_k\|^2 \right) + \left( \frac{L\tau_k}{2}- \frac{\mu}{2\tau_k}\right)\|x_{k+1} - y_k\|^2  + \left(\frac{\tau_k^2}{2\mu}-\frac{1}{2L}\right)\|\nabla f(x_{k+1})\|^2.   
\end{align*}
Thus, we have shown
\begin{equation}
\tilde E_{k+1} -\tilde  E_k \leq -\tau_k \tilde E_k + \varepsilon_{k+1}, 
\end{equation}
where 
\begin{equation}
 \varepsilon_{k+1}  = \left( \frac{L\tau_k}{2}- \frac{\mu}{2\tau_k}\right)\|x_{k+1} - y_k\|^2  + \left(\frac{\tau_k^2}{2\mu}-\frac{1}{2L}\right)\|\nabla f(x_{k+1})\|^2.   
\end{equation}
Choosing $\tau_k \leq 1/\sqrt{\kappa}$ ensures that $\varepsilon_{k+1} \leq 0$. 
%\end{proof}


\subsection{Proof of Accelerated Gradient \eqref{Eq:StrongConv2}}
%\begin{proof}
\begin{align*}
\tilde E_{k+1} -\tilde  E_k &= f(y_{k+1}) - f(y_k) - \mu \langle z_{k+1} - z_k, x^\ast - z_{k+1}\rangle - \frac{\mu}{2}\|z_{k+1} - z_k\|^2\\
&= f(y_{k+1}) - f(x_k) + f(x_k) - f(y_k) - \mu \langle z_{k+1} - z_k, x^\ast - z_{k}\rangle + \frac{\mu}{2}\|z_{k+1} - z_k\|^2\\
&= f(y_{k+1}) - f(x_k) + \langle \nabla f(x_k), x_k - y_k\rangle - \frac{\mu}{2}\|x_k - y_k\|^2 - \mu \langle z_{k+1} - z_k, x^\ast - z_{k}\rangle + \frac{\mu}{2}\|z_{k+1} - z_k\|^2\\
&= f(y_{k+1}) - f(x_k) + \langle \nabla f(x_k), x_k - y_k\rangle - \frac{\mu}{2}\|x_k - y_k\|^2 + \tau_k \langle \nabla f(x_{k}) - \mu(x_{k} - z_k), x^\ast - z_{k}\rangle \\
&\quad + \frac{\mu}{2}\|z_{k+1} - z_k\|^2\\
&\overset{\eqref{Eq:ZSeq1}\eqref{Eq:Coupling1}}{=} f(y_{k+1})- f(x_k) + \langle \nabla f(x_k), x_k - y_k\rangle - \frac{\mu}{2}\|x_k - y_k\|^2 + \tau_k \langle \nabla f(x_{k}), x^\ast - x_{k} \rangle\\
&\quad   - \tau_k\mu\langle x_{k} - z_k, x^\ast - z_{k}\rangle + \langle\nabla f(x_{k}), y_{k} - x_{k}\rangle + \frac{\mu}{2}\|z_{k+1} - z_k\|^2\\
%
&\leq -\tau_k\left( f(x_{k} ) - f(x^\ast) + \frac{\mu}{2}\|x^\ast - x_{k}\|^2 \right) + f(y_{k+1}) - f(x_k)  - \frac{\mu}{2}\|x_k - y_k\|^2   \\
&\quad - \tau_k\mu\langle x_{k} - z_k, x^\ast - z_{k}\rangle  + \frac{\mu}{2}\|z_{k+1} - z_k\|^2\\
&= -\tau_k\left( f(x_{k} ) - f(x^\ast) + \frac{\mu}{2}\|x^\ast - z_k\|^2 \right) + f(y_{k+1}) - f(x_k)  - \frac{\mu}{2}\|x_k - y_k\|^2  \\
&\quad   + \frac{\mu}{2}\|z_{k+1} - z_k\|^2 -\frac{\tau_k\mu}{2}\|x_k - z_k\|^2\\
&\overset{\eqref{Eq:ZSeq1}}{=} -\tau_k\left( f(x_k ) - f(x^\ast) + \frac{\mu}{2}\|x^\ast - z_k\|^2 \right) + f(y_{k+1}) - f(x_k)- \frac{\mu}{2}\|x_k - y_k\|^2\\
&\quad    + \frac{\mu}{2}\|\tau_k(x_{k} - z_k)\|^2 - \tau_k \langle \nabla f(x_{k}), \tau_k(x_{k} - z_k)\rangle + \frac{\tau_k^2}{2\mu}\|\nabla f(x_{k})\|^2-\frac{\mu}{2\tau_k}\|x_k - y_k\|^2\\
&\overset{\eqref{Eq:Coupling1}}{=} -\tau_k\left( f(x_k ) - f(x^\ast) + \frac{\mu}{2}\|x^\ast - z_k\|^2 \right) + f(y_{k+1}) - f(x_k)  \\
&\quad    - \tau_k \langle \nabla f(x_{k}), y_k - x_k\rangle + \frac{\tau_k^2}{2\mu}\|\nabla f(x_{k})\|^2 -\frac{\mu}{2\tau_k}\|x_k - y_k\|^2\\
&\leq -\tau_k\left( f(x_k ) - f(x^\ast) + \frac{\mu}{2}\|x^\ast - z_k\|^2 \right) + f(y_{k+1}) - f(x_k) - \tau_k(f(y_k) - f(x_k))+ \frac{\tau_k^2}{2\mu}\|\nabla f(x_{k})\|^2 \\
&\quad + \left(\frac{\tau_kL}{2}-\frac{\mu}{2\tau_k}\right)\|x_k - y_k\|^2\\
&= -\tau_k\left( f(y_k ) - f(x^\ast) + \frac{\mu}{2}\|x^\ast - z_k\|^2 \right) + f(y_{k+1}) - f(x_k) + \frac{\tau_k^2}{2\mu}\|\nabla f(x_{k})\|^2 \\
&\quad+ \left(\frac{\tau_kL}{2}-\frac{\mu}{2\tau_k}\right)\|x_k - y_k\|^2\\
&\leq -\tau_k\left( f(y_k ) - f(x^\ast) + \frac{\mu}{2}\|x^\ast - z_k\|^2 \right) + \left(\frac{\tau_k^2}{2\mu}-\frac{1}{2L}\right)\|\nabla f(x_k)\|^2 + \left(\frac{\tau_kL}{2}-\frac{\mu}{2\tau_k}\right)\|x_k - y_k\|^2.
\end{align*}
If we choose $\tau_k \leq 1/\sqrt{\kappa}$ then we have shown 
\begin{equation}
\tilde E_{k+1} - \tilde E_k \leq -\tau_k\tilde  E_k.
\end{equation}
\subsection{Derivation of Lyapunov Function}
For completeness, we derive the Lyapunov function for the dynamics~\eqref{Eq:StrongDyn}. From~\eqref{Eq:Strong}, the following inequality holds for~\eqref{Eq:StrongDyn}:
\begin{align*}
\frac{d}{dt}\left(e^{\beta_t}\frac{\mu}{2} \|x - Z_t\|^2\right)\, &=  \dot \beta_t e^{\beta_t}\left( f(X_t) + \langle \nabla f(X_t), x - X_t \rangle+ \frac{\mu}{2} \|x - X_t\|^2\right)   -  \frac{d}{dt}\left( e^{ \beta_t}  f(X_t)\right),\notag \\
&\leq \dot \beta_t e^{\beta_t} f(x) -  \frac{d}{dt}\left( e^{ \beta_t}  f(X_t)\right),\notag \\
& = -\frac{d}{dt} \left(e^{\beta_t}(f(X_t) - f(x)\right).
\end{align*}
Integrating and setting $x = x^\ast$ shows that~\eqref{eq:general-sc-lyap} is a Lyapunov function for the dynamics \eqref{Eq:StrongDyn}.
\section{Estimate Sequences}
\label{App:EstSeq}
\subsection{The Quasi-Montone Subgradient Method}
\paragraph{Equivalence in discrete time}
The discrete-time estimate sequence~\eqref{Eq:Est} for quasi-monotone subgradient method can be written:
\begin{align*}
\phi_{k+1}(x) - \frac{\tilde \varepsilon_{k+1}}{A_{k+1}} &:= f(x_{k+1}) + \frac{1}{A_{k+1}}D_h(x, z_{k+1}) - \frac{\tilde \varepsilon_{k+1}}{A_{k+1}}\\
& \overset{\eqref{Eq:Est}}{=} (1 - \tau_k) \left(\phi_k(x) - \frac{\tilde \varepsilon_k}{A_k}\right) + \tau_k f_k(x) \\
&= \left(1 - \frac{\alpha_k}{A_{k+1}} \right)\left(f(x_k) +\frac{1}{A_k} D_h(x, z_k) -\frac{\tilde \varepsilon_k}{A_k}\right) + \frac{\alpha_k}{A_{k+1}} f_k(x).
\end{align*}
Multiplying through by $A_{k+1}$, we have
\begin{align*}
A_{k+1} \left(f(x_{k+1}) + \frac{1}{A_{k+1}}D_h(x, z_{k+1}) - \frac{\tilde \varepsilon_{k+1}}{A_{k+1}}\right) &= (A_{k+1} - (A_{k+1} - A_k)) \left(f(x_k) + \frac{1}{A_k}D_h(x, z_k) - \frac{\tilde \varepsilon_k}{A_k}\right)\\
&\quad  + (A_{k+1} - A_k) f_k(x)\\
&=  A_k \left(f(x_k) + \frac{1}{A_k}D_h(x, z_k) - \frac{\tilde \varepsilon_k}{A_k}\right) + (A_{k+1} - A_k) f_k(x)\\
& \overset{\eqref{Eq:Under}}{\leq}  A_k f(x_k) + D_h(x, z_k) - \tilde \varepsilon_k + (A_{k+1} - A_k) f(x).
\end{align*}
Rearranging, we obtain our Lyapunov argument $E_{k+1} \leq E_k + \varepsilon_{k+1}$ for~\eqref{Eq:DiscLyapFunc1}:
\begin{align*}
A_{k+1}(f(x_{k+1}) - f(x)) + D_h(x, z_{k+1}) &\leq A_k (f(x_k) - f(x)) + D_h(x, z_k) + \varepsilon_{k+1},
\end{align*}
where $\varepsilon_{k+1} = \tilde \varepsilon_{k+1} - \tilde \varepsilon_k = \frac{A_{k+1} -A_k}{2}\|\nabla f(x_{k+1})\|^2$.

Going the other direction, from our Lyapunov analysis we can derive the following bound:
\begin{subequations}\label{Eq:EstSeqErr}
\begin{align}
E_k &\leq E_0 + \tilde \varepsilon_{k}\\
A_{k}(f(x_{k}) - f(x)) + D_h(x, z_{k}) &\leq A_0 (f(x_0) - f(x)) + D_h(x, z_0) + \tilde \varepsilon_{k}\notag\\
A_{k}\left(f(x_{k}) - \frac{1}{A_{k}}D_h(x, z_{k})\right) &\leq (A_k -  A_0) f(x) + A_0\left(f(x_0)+ \frac{1}{A_0}  D_h(x^\ast, z_0)\right) + \tilde \varepsilon_{k}\notag \\
A_k \phi_k(x) &\leq (A_k -A_0) f(x) + A_0 \phi_0(x) + \tilde \varepsilon_{k}.
\label{Eq:EstSeq}
\end{align}
\end{subequations}
Rearranging, we obtain our estimate sequence~\eqref{Eq:Ineq1} ($A_0 = 1$) with an additional error term:
\begin{subequations}
\begin{align}
\phi_k(x) &\leq \Big(1 -\frac{A_0}{A_k}\Big) f(x) + \frac{A_0}{A_k} \phi_0(x) + \frac{\tilde \varepsilon_{k}}{A_k}\\
&= \Big(1 - \frac{1}{A_k}\Big)f(x) + \frac{1}{A_k}\phi_0(x)+ \frac{\tilde \varepsilon_{k}}{A_k}.
\end{align}
\end{subequations}
Note, given accelerated gradient method and quasi-monotone subgradient method have the same continuous-time limit, the equivalence between the Lyapunov argument and method estimate sequences holds for this setting.
\subsection{The Conditional Gradient Method}
\paragraph{Equivalence in discrete time}
The discrete-time estimate sequence~\eqref{Eq:Est} for conditional gradient method can be written:
\begin{align*}
\phi_{k+1}(x) - \frac{\tilde \varepsilon_{k+1}}{A_{k+1}} := f(x_{k+1})  - \frac{\tilde \varepsilon_{k+1}}{A_{k+1}}&\overset{\eqref{Eq:Est}}{=} (1 - \tau_k) \left(\phi_k(x) - \frac{\tilde \varepsilon_k}{A_k}\right) + \tau_k f_k(x) \\
&\overset{\text{Table}~\ref{table:Table}}{=} \left(1 - \frac{\alpha_k}{A_{k+1}} \right)\left(f(x_k) -\frac{\tilde \varepsilon_k}{A_k}\right) + \frac{\alpha_k}{A_{k+1}} f_k(x).
\end{align*}
Multiplying through by $A_{k+1}$, we have
\begin{align*}
A_{k+1} \left(f(x_{k+1}) - \frac{\tilde \varepsilon_{k+1}}{A_{k+1}}\right) &= (A_{k+1} - (A_{k+1} - A_k)) \left(f(x_k) - \frac{\tilde \varepsilon_k}{A_k}\right) + (A_{k+1} - A_k) f_k(x)\\
&=  A_k \left(f(x_k) - \frac{\tilde \varepsilon_k}{A_k}\right) + (A_{k+1} - A_k) f_k(x)\\
& \overset{\eqref{Eq:Under}}{\leq}  A_k f(x_k)  - \tilde \varepsilon_k + (A_{k+1} - A_k) f(x).\
\end{align*}
Rearranging, we obtain our Lyapunov argument $E_{k+1} \leq E_k + \varepsilon_{k+1}$ for~\eqref{Eq:FrankWolfeLyap}:
\begin{align*}
A_{k+1}(f(x_{k+1}) - f(x))  &\leq A_k (f(x_k) - f(x)) + \varepsilon_{k+1},
\end{align*}
%also written $E_{k+1} \leq E_k + \varepsilon_{k+1}$ for \eqref{Eq:DiscLyapFunc1}, 
where $\varepsilon_{k+1} = \tilde \varepsilon_{k+1} - \tilde \varepsilon_k = \frac{L}{2}\frac {(A_{k+1} - A_k)^2}{A_{k+1}} diam(\X)^2$.
Going the other direction, from our Lyapunov analysis we can derive the following bound:
%\begin{align*}
%0 &\geq E_{k+1} - E_k - \varepsilon_k  \\
%& = A_{k+1}(f(x_{k+1}) - f(x)) + D_h(x, z_{k+1})  - A_k (f(x_k) - f(x)) - D_h(x, z_k)  -  \varepsilon_{k}\\
%& =  A_{k+1} \left(f(x_{k+1}) + \frac{1}{A_{k+1}} D_h(x, z_{k+1})\right) - A_k \left(f(x_k) +  \frac{1}{A_{k}} D_h(x, z_{k})\right) - (A_{k+1} - A_k)f(x) - \tilde \varepsilon_{k+1} - \tilde \varepsilon_k
%\end{align*}
\begin{subequations}
\begin{align*}
E_k &\leq E_0 + \tilde \varepsilon_{k}\\
A_{k}(f(x_{k}) - f(x)) &\leq A_0 (f(x_0) - f(x)) + \tilde \varepsilon_{k}\notag\\
A_{k}f(x_{k}) &\leq (A_k -  A_0) f(x) + A_0f(x_0) + \tilde \varepsilon_{k}\notag \\
A_k \phi_k(x) &\leq (A_k -A_0) f(x) + A_0 \phi_0(x) + \tilde \varepsilon_{k}\label{Eq:EstSeq}.
\end{align*}
\end{subequations}
Rearranging, we obtain our estimate sequence~\eqref{Eq:Ineq1} ($A_0 = 1$) with an additional error term:
\begin{subequations}
\begin{align}
\phi_k(x) &\leq \Big(1 -\frac{A_0}{A_k}\Big) f(x) + \frac{A_0}{A_k} \phi_0(x) + \frac{\tilde \varepsilon_{k}}{A_k}\\
&=\Big(1 - \frac{1}{A_k}\Big)f(x) + \frac{1}{A_k}\phi_0(x)+ \frac{\tilde \varepsilon_{k}}{A_k}.
\end{align}
\end{subequations}
\subsection{Accelerated Gradient Descent (Strong Convexity)}
The discrete-time estimate sequence~\eqref{Eq:Est} for accelerated gradient descent can be written:
\begin{align*}
\phi_{k+1}(x) &:= f(x_{k+1}) + \frac{\mu}{2}\|x -  z_{k+1}\|^2 \\
& \overset{\eqref{Eq:Est}}{=} (1 - \tau_k) \phi_k(x)  + \tau_k f_k(x) \\
&\overset{\eqref{Eq:Under}}{\leq}(1 - \tau_k) \phi_k(x)  + \tau_k f(x).
%&= \left(1 - \frac{\alpha_k}{A_{k+1}} \right)\left(f(x_k) +\frac{1}{A_k} D_h(x, z_k)\right) + \frac{\alpha_k}{A_{k+1}} f_k(x)
\end{align*}
Therefore, we obtain the inequality $E_{k+1} - E_k \leq -\tau_k E_k$ for our Lyapunov function~\eqref{Eq:LyapStrong}:
%Multiplying through by $A_{k+1}$, we have
\begin{align*}
\phi_{k+1}(x) - \phi_k(x) &\leq - \tau_k(\phi_k(x)  -  f(x))\\
f(x_{k+1})  + \frac{\mu}{2}\|x - z_{k+1}\|^2 - \left(f(x_k)   + \frac{\mu}{2}\|x - z_{k+1}\|^2\right) &\overset{\text{Table}~\ref{table:Table}}{\leq} - \tau_k \left(f(x_k) - f(x)  + \frac{\mu}{2}\|x - z_{k+1}\|^2\right)\\ 
f(x_{k+1}) -f(x) + \frac{\mu}{2}\|x - z_{k+1}\|^2 - \left(f(x_k) - f(x)  + \frac{\mu}{2}\|x - z_{k+1}\|^2\right) &\leq - \tau_k \left(f(x_k) - f(x)  + \frac{\mu}{2}\|x - z_{k+1}\|^2\right). 
%A_{k+1} \left(f(x_{k+1}) + \frac{1}{A_{k+1}}D_h(x, z_{k+1})\right) &= (A_{k+1} - (A_{k+1} - A_k)) \left(f(x_k) + \frac{1}{A_k}D_h(x, z_k)\right) + (A_{k+1} - A_k) f_k(x)\\
%&=  A_k \left(f(x_k) + \frac{1}{A_k}D_h(x, z_k) \right) + (A_{k+1} - A_k) f_k(x)\\
%& \overset{\eqref{Eq:Under}}{\leq}  A_k f(x_k) + D_h(x, z_k)+ (A_{k+1} - A_k) f(x)\
\end{align*}
%Rearranging, we obtain the inequality $E_{k+1} \leq E_k$ for our Lyapunov function~\eqref{Eq:LyapStrong}. 
Going the other direction, we have, 
\begin{align*}
E_{k+1} - E_k &\leq -\tau_k E_k\\
\phi_{k+1} &\leq (1 - \tau_k) \phi_k(x)  + \tau_k f(x)\\
A_{k+1} \phi_{k+1} &\leq A_k \phi_k + (A_{k+1} - A_k) f(x).
\end{align*}
Summing over the right-hand side, we obtain the estimate sequence~\eqref{Eq:Ineq1}: 
\begin{align*}
\phi_{k+1} &\leq \Big(1 -\frac{A_0}{A_{k+1}}\Big) f(x) + \frac{A_0}{A_{k+1}} \phi_0(x)\\
& =  \Big(1 -\frac{1}{A_{k+1}}\Big) f(x) + \frac{1}{A_{k+1}} \phi_0(x).\\
\end{align*}


\end{document}


